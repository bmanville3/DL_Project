{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-PVsZeWjCiw"
   },
   "source": [
    "### **Axial LOB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVPONVeVw0nh",
    "outputId": "5301a18a-5c77-4648-f73a-63eca27d0f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "#Load necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (203800, 40)\n",
      "Validation data shape: (50950, 40)\n",
      "Testing data shape: (139587, 40)\n"
     ]
    }
   ],
   "source": [
    "#2 \n",
    "#Import necessary packages\n",
    "import numpy as np\n",
    "\n",
    "# Define paths based on your directory structure for NoAuction\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load training and validation data\n",
    "dec_data = np.loadtxt(train_file)\n",
    "dec_train = dec_data[:, :int(dec_data.shape[1] * 0.8)]\n",
    "dec_val = dec_data[:, int(dec_data.shape[1] * 0.8):]\n",
    "\n",
    "# Load test data and concatenate\n",
    "dec_test1 = np.loadtxt(test_file1)\n",
    "dec_test2 = np.loadtxt(test_file2)\n",
    "dec_test3 = np.loadtxt(test_file3)\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# Set parameters\n",
    "W = 40                     # Number of features\n",
    "dim = 40                   # Number of LOB states\n",
    "horizon = 2                # Horizon for target calculation\n",
    "T = 5                      # Time window size for dataset creation\n",
    "\n",
    "# Prepare labels\n",
    "y_train = dec_train[-horizon, :].flatten()\n",
    "y_val = dec_val[-horizon, :].flatten()\n",
    "y_test = dec_test[-horizon, :].flatten()\n",
    "\n",
    "# Adjust labels for training, validation, and test sets\n",
    "y_train = y_train[dim-1:] - 1\n",
    "y_val = y_val[dim-1:] - 1\n",
    "y_test = y_test[dim-1:] - 1 \n",
    "\n",
    "# Prepare data for model input\n",
    "dec_train = dec_train[:40, :].T\n",
    "dec_val = dec_val[:40, :].T\n",
    "dec_test = dec_test[:40, :].T\n",
    "\n",
    "# Print shapes to verify data\n",
    "print(\"Training data shape:\", dec_train.shape)\n",
    "print(\"Validation data shape:\", dec_val.shape)\n",
    "print(\"Testing data shape:\", dec_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 203756\n",
      "Dataset length: 50906\n",
      "Dataset length: 139543\n",
      "Input batch shape: torch.Size([64, 1, 40, 40])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, x, y, num_classes, dim):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.x = x   \n",
    "        self.y = y\n",
    "\n",
    "        # Compute length based on rolling window\n",
    "        self.length = x.shape[0] - T - self.dim + 1\n",
    "        print(\"Dataset length:\", self.length)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x = torch.from_numpy(x).float()  # Ensure data is float for model input\n",
    "        self.x = torch.unsqueeze(x, 1)   # Add channel dimension\n",
    "        self.y = torch.from_numpy(y).long()  # Labels should be long type for classification\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Extract input with rolling window and adjust shape\n",
    "        input = self.x[i:i+self.dim, :]\n",
    "        input = input.permute(1, 0, 2)  # Adjust to expected shape [1, dim, features]\n",
    "        \n",
    "        return input, self.y[i]\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 64\n",
    "num_classes = 3  # Adjust based on your problem (e.g., 3 classes for LOB levels)\n",
    "dim = 40  # Number of LOB states, adjust as needed\n",
    "\n",
    "# Instantiate Dataset objects for train, validation, and test sets\n",
    "dataset_train = Dataset(dec_train, y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val, y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test, y_test, num_classes, dim)\n",
    "\n",
    "# Create DataLoader objects for batching and shuffling\n",
    "train_loader = data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify DataLoader functionality\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input batch shape:\", inputs.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    break  # Test with a single batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (203759, 40)\n",
      "Training labels shape: (203759,)\n",
      "Validation data shape: (50909, 40)\n",
      "Validation labels shape: (50909,)\n",
      "Testing data shape: (139546, 40)\n",
      "Testing labels shape: (139546,)\n",
      "Dataset length: 203756\n",
      "Dataset length: 50906\n",
      "Dataset length: 139543\n",
      "Sample input batch shape: torch.Size([64, 1, 40, 40])\n",
      "Sample label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "# Import necessary packages\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 50 \n",
    "c_final = 4              # Channel output size of the second conv layer\n",
    "n_heads = 4\n",
    "c_in_axial = 32          # Channel output size of the first conv layer\n",
    "c_out_axial = 32\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# Adjust label preparation without flattening the entire dataset\n",
    "horizon = 2\n",
    "dim = 40\n",
    "\n",
    "# Define lengths based on data size and required horizon offset\n",
    "train_len = dec_train.shape[0] - dim + 1 - horizon\n",
    "val_len = dec_val.shape[0] - dim + 1 - horizon\n",
    "test_len = dec_test.shape[0] - dim + 1 - horizon\n",
    "\n",
    "# Slicing the labels to match the data lengths exactly\n",
    "y_train = dec_train[dim-1:dim-1 + train_len, -horizon] - 1\n",
    "y_val = dec_val[dim-1:dim-1 + val_len, -horizon] - 1\n",
    "y_test = dec_test[dim-1:dim-1 + test_len, -horizon] - 1\n",
    "\n",
    "# Confirm alignment\n",
    "print(\"Training data shape:\", dec_train[:train_len].shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Validation data shape:\", dec_val[:val_len].shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)\n",
    "print(\"Testing data shape:\", dec_test[:test_len].shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)\n",
    "\n",
    "# Create Dataset instances\n",
    "dataset_train = Dataset(dec_train, y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val, y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test, y_test, num_classes, dim)\n",
    "\n",
    "# Set up DataLoader instances\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify DataLoader functionality with a sample batch\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Sample input batch shape:\", inputs.shape)\n",
    "    print(\"Sample label batch shape:\", labels.shape)\n",
    "    break  # Test with a single batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 203715\n",
      "Dataset length: 50865\n",
      "Dataset length: 139502\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "# Import necessary packages\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 50 \n",
    "c_final = 4\n",
    "n_heads = 4\n",
    "c_in_axial = 32\n",
    "c_out_axial = 32\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "num_classes = 3\n",
    "\n",
    "# Ensure labels align with data by slicing both consistently\n",
    "horizon = 2\n",
    "dim = 40\n",
    "train_len = dec_train.shape[0] - dim + 1 - horizon\n",
    "val_len = dec_val.shape[0] - dim + 1 - horizon\n",
    "test_len = dec_test.shape[0] - dim + 1 - horizon\n",
    "y_train = dec_train[dim-1:dim-1 + train_len, -horizon] - 1\n",
    "y_val = dec_val[dim-1:dim-1 + val_len, -horizon] - 1\n",
    "y_test = dec_test[dim-1:dim-1 + test_len, -horizon] - 1\n",
    "\n",
    "# Dataset setup\n",
    "dataset_train = Dataset(dec_train[:train_len], y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val[:val_len], y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test[:test_len], y_test, num_classes, dim)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Architecture\n",
    "def _conv1d1x1(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(out_channels)\n",
    "    )\n",
    "\n",
    "class GatedAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads, dim, flag):\n",
    "        super().__init__()\n",
    "        assert (in_channels % heads == 0) and (out_channels % heads == 0)\n",
    "        self.in_channels, self.out_channels, self.heads = in_channels, out_channels, heads\n",
    "        self.dim_head_v = out_channels // heads\n",
    "        self.flag, self.dim = flag, dim\n",
    "        self.dim_head_qk = self.dim_head_v // 2\n",
    "        self.qkv_channels = self.dim_head_v + self.dim_head_qk * 2\n",
    "\n",
    "        self.to_qkv = _conv1d1x1(in_channels, heads * self.qkv_channels)\n",
    "        self.bn_qkv, self.bn_similarity = nn.BatchNorm1d(heads * self.qkv_channels), nn.BatchNorm2d(heads * 3)\n",
    "        self.bn_output = nn.BatchNorm1d(heads * self.qkv_channels)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.f_qr, self.f_kr = nn.Parameter(torch.tensor(0.3), False), nn.Parameter(torch.tensor(0.3), False)\n",
    "        self.f_sve, self.f_sv = nn.Parameter(torch.tensor(0.3), False), nn.Parameter(torch.tensor(0.5), False)\n",
    "\n",
    "        # Position embedding\n",
    "        self.relative = nn.Parameter(torch.randn(self.dim_head_v * 2, dim * 2 - 1), requires_grad=True)\n",
    "        query_index = torch.arange(dim).unsqueeze(0)\n",
    "        key_index = torch.arange(dim).unsqueeze(1)\n",
    "        relative_index = key_index - query_index + dim - 1\n",
    "        self.register_buffer('flatten_index', relative_index.view(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.flag:\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n",
    "        N, W, C, H = x.shape\n",
    "        x = x.view(N * W, C, H)\n",
    "\n",
    "        x = self.to_qkv(x)\n",
    "        qkv = self.bn_qkv(x)\n",
    "        q, k, v = torch.split(qkv.reshape(N * W, self.heads, self.dim_head_v * 2, H),\n",
    "                              [self.dim_head_v // 2, self.dim_head_v // 2, self.dim_head_v], dim=2)\n",
    "\n",
    "        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.dim_head_v * 2, self.dim, self.dim)\n",
    "        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.dim_head_qk, self.dim_head_qk, self.dim_head_v], dim=0)\n",
    "        qr, kr, qk = torch.einsum('bgci,cij->bgij', q, q_embedding), torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3), torch.einsum('bgci, bgcj->bgij', q, k)\n",
    "        qr, kr = torch.mul(qr, self.f_qr), torch.mul(kr, self.f_kr)\n",
    "\n",
    "        similarity = torch.softmax(self.bn_similarity(torch.cat([qk, qr, kr], dim=1)).view(N * W, 3, self.heads, H, H).sum(dim=1), dim=3)\n",
    "        sv, sve = torch.mul(torch.einsum('bgij,bgcj->bgci', similarity, v), self.f_sv), torch.mul(torch.einsum('bgij,cij->bgci', similarity, v_embedding), self.f_sve)\n",
    "\n",
    "        output = self.bn_output(torch.cat([sv, sve], dim=-1).view(N * W, self.out_channels * 2, H)).view(N, W, self.out_channels, 2, H).sum(dim=-2)\n",
    "        return output.permute(0, 2, 3, 1) if not self.flag else output.permute(0, 2, 1, 3)\n",
    "\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, n_heads, pool_kernel, pool_stride):\n",
    "        super().__init__()\n",
    "        self.CNN_in, self.CNN_out = nn.Conv2d(1, c_in, kernel_size=1), nn.Conv2d(c_out, c_final, kernel_size=1)\n",
    "        self.CNN_res2, self.CNN_res1 = nn.Conv2d(c_out, c_final, kernel_size=1), nn.Conv2d(1, c_out, kernel_size=1)\n",
    "        self.norm, self.res_norm2, self.res_norm1, self.norm2 = nn.BatchNorm2d(c_in), nn.BatchNorm2d(c_final), nn.BatchNorm2d(c_out), nn.BatchNorm2d(c_final)\n",
    "        self.axial_height_1, self.axial_width_1 = GatedAxialAttention(c_out, c_out, n_heads, H, False), GatedAxialAttention(c_out, c_out, n_heads, W, True)\n",
    "        self.axial_height_2, self.axial_width_2 = GatedAxialAttention(c_out, c_out, n_heads, H, False), GatedAxialAttention(c_out, c_out, n_heads, W, True)\n",
    "        self.activation, self.linear = nn.ReLU(), nn.Linear(1600, 3)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.activation(self.norm(self.CNN_in(x)))\n",
    "        y, x = self.axial_width_1(y), self.activation(self.res_norm1(self.CNN_res1(x)))\n",
    "        y, y_copy = y + x, y.detach().clone()\n",
    "        y = self.axial_width_2(y + x)\n",
    "        y = self.activation(self.res_norm2(self.CNN_out(self.axial_height_2(self.axial_height_1(y)))))\n",
    "        return torch.softmax(self.linear(torch.flatten(self.pooling(y + self.activation(self.norm2(self.CNN_res2(y_copy)))), 1)), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training data shape: (149, 254750)\n",
      "Test data combined shape: (447, 55478)\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "# Import necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Clean lines: Remove extra spaces and ensure proper formatting\n",
    "    cleaned_lines = [line.strip().replace('   ', ' ').replace('  ', ' ') for line in lines]\n",
    "    # Load as NumPy array\n",
    "    data = np.genfromtxt(cleaned_lines, delimiter=' ')\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "dec_train = load_data(train_file)\n",
    "dec_test1 = load_data(test_file1)\n",
    "dec_test2 = load_data(test_file2)\n",
    "dec_test3 = load_data(test_file3)\n",
    "\n",
    "print(\"Training data shape:\", dec_train.shape)\n",
    "\n",
    "# Ensure the test data is padded consistently\n",
    "def pad_array(arr, target_cols):\n",
    "    padding = target_cols - arr.shape[1]\n",
    "    return np.pad(arr, ((0, 0), (0, padding)), mode='constant')\n",
    "\n",
    "def pad_arrays(*arrays):\n",
    "    max_cols = max(arr.shape[1] for arr in arrays)\n",
    "    return [pad_array(arr, max_cols) for arr in arrays]\n",
    "\n",
    "dec_test1, dec_test2, dec_test3 = pad_arrays(dec_test1, dec_test2, dec_test3)\n",
    "test_data_combined = np.vstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# Parameters\n",
    "W, dim, num_classes, batch_size = 40, 40, 3, 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon, W, num_classes):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(data.shape[0] - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :W]\n",
    "        label = int(data[i + dim + horizon - 1, -1])\n",
    "        if 0 <= label < num_classes:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Test with AxialLOB model\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, c_in, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(c_in, c_out, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(c_out, c_final, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(c_final, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = AxialLOB(W, dim, 16, 16, 16, num_classes).to(device)\n",
    "\n",
    "# Print test data shape for debugging\n",
    "print(\"Test data combined shape:\", test_data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Evaluating CNN\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Evaluating AxialLOB\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Evaluating DeepLOB\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "       Model        Prediction Horizon  Precision (%)  Recall (%)     F1 (%)\n",
      "0        CNN   Prediction Horizon = 10      31.476793   33.333333  32.378472\n",
      "1        CNN   Prediction Horizon = 20      32.207792   33.333333  32.760898\n",
      "2        CNN   Prediction Horizon = 30      32.355556   33.333333  32.837167\n",
      "3        CNN   Prediction Horizon = 50      32.677903   33.333333  33.002364\n",
      "4        CNN  Prediction Horizon = 100      32.792208   33.333333  33.060556\n",
      "5   AxialLOB   Prediction Horizon = 10      31.476793   33.333333  32.378472\n",
      "6   AxialLOB   Prediction Horizon = 20      32.207792   33.333333  32.760898\n",
      "7   AxialLOB   Prediction Horizon = 30      32.355556   33.333333  32.837167\n",
      "8   AxialLOB   Prediction Horizon = 50      32.677903   33.333333  33.002364\n",
      "9   AxialLOB  Prediction Horizon = 100      32.792208   33.333333  33.060556\n",
      "10   DeepLOB   Prediction Horizon = 10      41.227189   46.188561  42.976084\n",
      "11   DeepLOB   Prediction Horizon = 20      32.207792   33.333333  32.760898\n",
      "12   DeepLOB   Prediction Horizon = 30      32.355556   33.333333  32.837167\n",
      "13   DeepLOB   Prediction Horizon = 50      32.676056   33.237822  32.954545\n",
      "14   DeepLOB  Prediction Horizon = 100      34.556575   55.225523  28.787505\n"
     ]
    }
   ],
   "source": [
    "#7 CNN, Axial LOB and Deep LOB - F1 scores\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "dec_train = load_data(train_file)\n",
    "dec_test1 = load_data(test_file1)\n",
    "dec_test2 = load_data(test_file2)\n",
    "dec_test3 = load_data(test_file3)\n",
    "\n",
    "# Preprocess to handle mismatched dimensions\n",
    "def preprocess_data(*arrays):\n",
    "    max_cols = max(array.shape[1] for array in arrays)\n",
    "    processed_arrays = []\n",
    "    for array in arrays:\n",
    "        if array.shape[1] < max_cols:\n",
    "            padded = np.pad(array, ((0, 0), (0, max_cols - array.shape[1])), mode='constant', constant_values=0)\n",
    "            processed_arrays.append(padded)\n",
    "        elif array.shape[1] > max_cols:\n",
    "            processed_arrays.append(array[:, :max_cols])\n",
    "        else:\n",
    "            processed_arrays.append(array)\n",
    "    return processed_arrays\n",
    "\n",
    "dec_test1, dec_test2, dec_test3 = preprocess_data(dec_test1, dec_test2, dec_test3)\n",
    "\n",
    "# Combine test data\n",
    "test_data_combined = np.vstack([dec_test1, dec_test2, dec_test3])\n",
    "\n",
    "# Parameters\n",
    "W, dim, num_classes, batch_size = 40, 40, 3, 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon, W, num_classes):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(data.shape[0] - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :W]\n",
    "        label = int(data[i + dim + horizon - 1, -1])\n",
    "        if 0 <= label < num_classes:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Models\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, W, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, c_in, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(c_in)\n",
    "        self.conv2 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "        self.conv3 = nn.Conv2d(c_out, c_final, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(c_final)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c_final, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class DeepLOB(nn.Module):\n",
    "    def __init__(self, W, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.lstm = nn.LSTM(W * 128, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, height, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_all(models, train_data, test_data, horizons):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}\")\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        for horizon in horizons:\n",
    "            print(f\"Evaluating for Prediction Horizon: {horizon}\")\n",
    "            X_train, y_train = create_sequences(train_data, dim, horizon, W, num_classes)\n",
    "            X_test, y_test = create_sequences(test_data, dim, horizon, W, num_classes)\n",
    "\n",
    "            train_loader = data.DataLoader(Dataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "            test_loader = data.DataLoader(Dataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Train\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(all_preds)\n",
    "            labels = np.concatenate(all_labels)\n",
    "            precision = precision_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            recall = recall_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            f1 = f1_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "\n",
    "            results.append([model_name, f\"Prediction Horizon = {horizon}\", precision, recall, f1])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Instantiate models\n",
    "models = {\n",
    "    \"CNN\": CNN(W, num_classes),\n",
    "    \"AxialLOB\": AxialLOB(W, dim, 16, 16, 16, num_classes),\n",
    "    \"DeepLOB\": DeepLOB(W, 64, num_classes),\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_all(models, dec_train, test_data_combined, [10, 20, 30, 50, 100])\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Prediction Horizon\", \"Precision (%)\", \"Recall (%)\", \"F1 (%)\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Evaluating B_TABL\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Evaluating C_TABL\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "    Model        Prediction Horizon  Precision (%)  Recall (%)     F1 (%)\n",
      "0  B_TABL   Prediction Horizon = 10      37.788537   55.406613  34.849938\n",
      "1  B_TABL   Prediction Horizon = 20      33.338251   41.829586  28.227732\n",
      "2  B_TABL   Prediction Horizon = 30      33.931624   31.359381  32.081888\n",
      "3  B_TABL   Prediction Horizon = 50      32.671082   28.271251  30.312340\n",
      "4  B_TABL  Prediction Horizon = 100      35.111111   55.445545  29.962829\n",
      "5  C_TABL   Prediction Horizon = 10      36.345008   43.923146  36.271320\n",
      "6  C_TABL   Prediction Horizon = 20      32.168459   32.168459  32.168459\n",
      "7  C_TABL   Prediction Horizon = 30      34.488162   35.663411  34.847395\n",
      "8  C_TABL   Prediction Horizon = 50      32.674200   33.142311  32.906591\n",
      "9  C_TABL  Prediction Horizon = 100      34.151913   47.552255  28.736825\n"
     ]
    }
   ],
   "source": [
    "# 8 B (tabl) and C (tabl) - F1 scores \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "dec_train = load_data(train_file)\n",
    "dec_test1 = load_data(test_file1)\n",
    "dec_test2 = load_data(test_file2)\n",
    "dec_test3 = load_data(test_file3)\n",
    "\n",
    "# Preprocess to handle mismatched dimensions\n",
    "def preprocess_data(*arrays):\n",
    "    max_cols = max(array.shape[1] for array in arrays)\n",
    "    processed_arrays = []\n",
    "    for array in arrays:\n",
    "        if array.shape[1] < max_cols:\n",
    "            padded = np.pad(array, ((0, 0), (0, max_cols - array.shape[1])), mode='constant', constant_values=0)\n",
    "            processed_arrays.append(padded)\n",
    "        elif array.shape[1] > max_cols:\n",
    "            processed_arrays.append(array[:, :max_cols])\n",
    "        else:\n",
    "            processed_arrays.append(array)\n",
    "    return processed_arrays\n",
    "\n",
    "dec_test1, dec_test2, dec_test3 = preprocess_data(dec_test1, dec_test2, dec_test3)\n",
    "\n",
    "# Combine test data\n",
    "test_data_combined = np.vstack([dec_test1, dec_test2, dec_test3])\n",
    "\n",
    "# Parameters\n",
    "W, dim, num_classes, batch_size = 40, 40, 3, 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon, W, num_classes):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(data.shape[0] - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :W]\n",
    "        label = int(data[i + dim + horizon - 1, -1])\n",
    "        if 0 <= label < num_classes:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Models\n",
    "class B_TABL(nn.Module):\n",
    "    def __init__(self, W, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(W * W, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class C_TABL(nn.Module):\n",
    "    def __init__(self, W, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(W * W, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_all(models, train_data, test_data, horizons):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}\")\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        for horizon in horizons:\n",
    "            print(f\"Evaluating for Prediction Horizon: {horizon}\")\n",
    "            X_train, y_train = create_sequences(train_data, dim, horizon, W, num_classes)\n",
    "            X_test, y_test = create_sequences(test_data, dim, horizon, W, num_classes)\n",
    "\n",
    "            train_loader = data.DataLoader(Dataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "            test_loader = data.DataLoader(Dataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Train\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(all_preds)\n",
    "            labels = np.concatenate(all_labels)\n",
    "            precision = precision_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            recall = recall_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            f1 = f1_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "\n",
    "            results.append([model_name, f\"Prediction Horizon = {horizon}\", precision, recall, f1])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Instantiate models\n",
    "models = {\n",
    "    \"B_TABL\": B_TABL(W, num_classes),\n",
    "    \"C_TABL\": C_TABL(W, num_classes),\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_all(models, dec_train, test_data_combined, [10, 20, 30, 50, 100])\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Prediction Horizon\", \"Precision (%)\", \"Recall (%)\", \"F1 (%)\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Evaluating DeepLOB-Seq2Seq\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Evaluating DeepLOB-Attention\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Evaluating for Prediction Horizon: 100\n",
      "               Model        Prediction Horizon  Precision (%)  Recall (%)  \\\n",
      "0    DeepLOB-Seq2Seq   Prediction Horizon = 10      31.476793   33.333333   \n",
      "1    DeepLOB-Seq2Seq   Prediction Horizon = 20      32.207792   33.333333   \n",
      "2    DeepLOB-Seq2Seq   Prediction Horizon = 30      32.355556   33.333333   \n",
      "3    DeepLOB-Seq2Seq   Prediction Horizon = 50      32.677903   33.333333   \n",
      "4    DeepLOB-Seq2Seq  Prediction Horizon = 100      32.792208   33.333333   \n",
      "5  DeepLOB-Attention   Prediction Horizon = 10      31.467345   33.154602   \n",
      "6  DeepLOB-Attention   Prediction Horizon = 20      32.207792   33.333333   \n",
      "7  DeepLOB-Attention   Prediction Horizon = 30      32.355556   33.333333   \n",
      "8  DeepLOB-Attention   Prediction Horizon = 50      32.677903   33.333333   \n",
      "9  DeepLOB-Attention  Prediction Horizon = 100      32.792208   33.333333   \n",
      "\n",
      "      F1 (%)  \n",
      "0  32.378472  \n",
      "1  32.760898  \n",
      "2  32.837167  \n",
      "3  33.002364  \n",
      "4  33.060556  \n",
      "5  32.288947  \n",
      "6  32.760898  \n",
      "7  32.837167  \n",
      "8  33.002364  \n",
      "9  33.060556  \n"
     ]
    }
   ],
   "source": [
    "# 9 Deep LOB seq2seq and deep LOB attention- F1 scores\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "dec_train = load_data(train_file)\n",
    "dec_test1 = load_data(test_file1)\n",
    "dec_test2 = load_data(test_file2)\n",
    "dec_test3 = load_data(test_file3)\n",
    "\n",
    "# Preprocess to handle mismatched dimensions\n",
    "def preprocess_data(*arrays):\n",
    "    max_cols = max(array.shape[1] for array in arrays)\n",
    "    processed_arrays = []\n",
    "    for array in arrays:\n",
    "        if array.shape[1] < max_cols:\n",
    "            padded = np.pad(array, ((0, 0), (0, max_cols - array.shape[1])), mode='constant', constant_values=0)\n",
    "            processed_arrays.append(padded)\n",
    "        elif array.shape[1] > max_cols:\n",
    "            processed_arrays.append(array[:, :max_cols])\n",
    "        else:\n",
    "            processed_arrays.append(array)\n",
    "    return processed_arrays\n",
    "\n",
    "dec_test1, dec_test2, dec_test3 = preprocess_data(dec_test1, dec_test2, dec_test3)\n",
    "\n",
    "# Combine test data\n",
    "test_data_combined = np.vstack([dec_test1, dec_test2, dec_test3])\n",
    "\n",
    "# Parameters\n",
    "W, dim, num_classes, batch_size = 40, 40, 3, 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon, W, num_classes):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(data.shape[0] - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :W]\n",
    "        label = int(data[i + dim + horizon - 1, -1])\n",
    "        if 0 <= label < num_classes:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Models\n",
    "class DeepLOBSeq2Seq(nn.Module):\n",
    "    def __init__(self, W, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(W, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.LSTM(W, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, _ = x.size()\n",
    "        x = x.view(batch_size, height, -1)  # Flatten spatial dimensions\n",
    "        _, (hidden, _) = self.encoder(x)  # Encoder outputs\n",
    "        x, _ = self.decoder(x)  # Decoder processes input\n",
    "        x = x[:, -1, :]  # Use last time-step\n",
    "        return self.fc(x)\n",
    "\n",
    "class DeepLOBAttention(nn.Module):\n",
    "    def __init__(self, W, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(W, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, _ = x.size()\n",
    "        x = x.view(batch_size, height, -1)  # Flatten spatial dimensions\n",
    "        x, _ = self.encoder(x)  # Encoder outputs\n",
    "        attention_weights = self.attention(x)  # Compute attention weights\n",
    "        x = torch.sum(attention_weights * x, dim=1)  # Apply attention\n",
    "        return self.fc(x)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_all(models, train_data, test_data, horizons):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}\")\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        for horizon in horizons:\n",
    "            print(f\"Evaluating for Prediction Horizon: {horizon}\")\n",
    "            X_train, y_train = create_sequences(train_data, dim, horizon, W, num_classes)\n",
    "            X_test, y_test = create_sequences(test_data, dim, horizon, W, num_classes)\n",
    "\n",
    "            train_loader = data.DataLoader(Dataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "            test_loader = data.DataLoader(Dataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Train\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(all_preds)\n",
    "            labels = np.concatenate(all_labels)\n",
    "            precision = precision_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            recall = recall_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "            f1 = f1_score(labels, preds, average='macro', zero_division=0) * 100\n",
    "\n",
    "            results.append([model_name, f\"Prediction Horizon = {horizon}\", precision, recall, f1])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Instantiate models\n",
    "models = {\n",
    "    \"DeepLOB-Seq2Seq\": DeepLOBSeq2Seq(W, 64, num_classes),\n",
    "    \"DeepLOB-Attention\": DeepLOBAttention(W, 64, num_classes),\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_all(models, dec_train, test_data_combined, [10, 20, 30, 50, 100])\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Prediction Horizon\", \"Precision (%)\", \"Recall (%)\", \"F1 (%)\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Label distribution in training set: {0: 72, 1: 23, 2: 3}\n",
      "Label distribution in test set: {0: 398}\n",
      "Epoch 1, Loss: 1.0942\n",
      "Epoch 2, Loss: 1.0723\n",
      "Epoch 3, Loss: 1.0747\n",
      "Epoch 4, Loss: 1.0394\n",
      "Epoch 5, Loss: 1.0468\n",
      "Epoch 6, Loss: 1.0325\n",
      "Epoch 7, Loss: 1.0241\n",
      "Epoch 8, Loss: 1.0100\n",
      "Epoch 9, Loss: 1.0265\n",
      "Epoch 10, Loss: 1.0327\n",
      "Predictions: [1 1 1 1 1 1 1 1 1 1] ... [1 1 1 1 1 1 1 1 1 1] (first 10 and last 10 predictions)\n",
      "Initial cash: 10000, transaction cost: 0.001\n",
      "SELL at step 34: Sold 0.00 shares at -0.96, Cash: 10000.00\n",
      "SELL at step 35: Sold 0.00 shares at 2.40, Cash: 10000.00\n",
      "SELL at step 36: Sold 0.00 shares at 0.38, Cash: 10000.00\n",
      "SELL at step 37: Sold 0.00 shares at -0.04, Cash: 10000.00\n",
      "SELL at step 38: Sold 0.00 shares at -0.00, Cash: 10000.00\n",
      "SELL at step 39: Sold 0.00 shares at 3.36, Cash: 10000.00\n",
      "SELL at step 40: Sold 0.00 shares at -0.13, Cash: 10000.00\n",
      "SELL at step 41: Sold 0.00 shares at -0.06, Cash: 10000.00\n",
      "SELL at step 42: Sold 0.00 shares at 0.01, Cash: 10000.00\n",
      "SELL at step 43: Sold 0.00 shares at 1.50, Cash: 10000.00\n",
      "SELL at step 44: Sold 0.00 shares at -1.05, Cash: 10000.00\n",
      "SELL at step 45: Sold 0.00 shares at -0.06, Cash: 10000.00\n",
      "SELL at step 46: Sold 0.00 shares at 0.01, Cash: 10000.00\n",
      "SELL at step 47: Sold 0.00 shares at 2.53, Cash: 10000.00\n",
      "Final Portfolio Value: 10000.00, Remaining Cash: 10000.00, Remaining Stock: 0.00\n",
      "\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Label distribution in training set: {0: 71, 1: 14, 2: 3}\n",
      "Label distribution in test set: {0: 388}\n",
      "Epoch 1, Loss: 1.0162\n",
      "Epoch 2, Loss: 1.0379\n",
      "Epoch 3, Loss: 1.0115\n",
      "Epoch 4, Loss: 1.0135\n",
      "Epoch 5, Loss: 0.9685\n",
      "Epoch 6, Loss: 0.9811\n",
      "Epoch 7, Loss: 0.9390\n",
      "Epoch 8, Loss: 0.9097\n",
      "Epoch 9, Loss: 1.0119\n",
      "Epoch 10, Loss: 0.8901\n",
      "Predictions: [1 1 1 1 1 1 1 1 1 1] ... [2 2 1 1 1 1 1 1 1 1] (first 10 and last 10 predictions)\n",
      "Initial cash: 10000, transaction cost: 0.001\n",
      "SELL at step 18: Sold 0.00 shares at 0.61, Cash: 10000.00\n",
      "SELL at step 19: Sold 0.00 shares at 0.52, Cash: 10000.00\n",
      "SELL at step 20: Sold 0.00 shares at 1.31, Cash: 10000.00\n",
      "SELL at step 21: Sold 0.00 shares at 0.39, Cash: 10000.00\n",
      "SELL at step 22: Sold 0.00 shares at 0.37, Cash: 10000.00\n",
      "SELL at step 23: Sold 0.00 shares at -1.06, Cash: 10000.00\n",
      "SELL at step 24: Sold 0.00 shares at -0.96, Cash: 10000.00\n",
      "SELL at step 25: Sold 0.00 shares at 2.40, Cash: 10000.00\n",
      "SELL at step 26: Sold 0.00 shares at 0.38, Cash: 10000.00\n",
      "SELL at step 27: Sold 0.00 shares at -0.04, Cash: 10000.00\n",
      "SELL at step 28: Sold 0.00 shares at -0.00, Cash: 10000.00\n",
      "SELL at step 29: Sold 0.00 shares at 3.36, Cash: 10000.00\n",
      "BUY at step 30: Bought -75989.74 shares at -0.13, Cash: -0.00, Stock: -75989.74\n",
      "BUY at step 31: Bought 0.00 shares at -0.06, Cash: -0.00, Stock: -75989.74\n",
      "BUY at step 32: Bought -0.00 shares at 0.01, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 33: Bought 0.00 shares at 1.50, Cash: -0.00, Stock: -75989.74\n",
      "BUY at step 34: Bought 0.00 shares at -1.05, Cash: -0.00, Stock: -75989.74\n",
      "BUY at step 35: Bought 0.00 shares at -0.06, Cash: -0.00, Stock: -75989.74\n",
      "BUY at step 36: Bought -0.00 shares at 0.01, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 37: Bought 0.00 shares at 2.53, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 38: Bought 0.00 shares at 0.46, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 39: Bought 0.00 shares at 0.08, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 40: Bought 0.00 shares at 0.06, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 41: Bought 0.00 shares at 1.14, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 42: Bought -0.00 shares at -1.26, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 43: Bought 0.00 shares at 0.09, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 44: Bought -0.00 shares at -0.02, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 45: Bought 0.00 shares at 2.33, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 46: Bought 0.00 shares at 1.80, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 47: Bought 0.00 shares at 0.16, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 48: Bought 0.00 shares at 0.03, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 49: Bought -0.00 shares at -0.33, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 50: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 51: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 52: Bought 0.00 shares at 0.03, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 53: Bought -0.00 shares at -0.56, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 54: Bought -0.00 shares at -0.04, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 55: Bought 0.00 shares at 0.10, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 56: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 57: Bought -0.00 shares at -0.74, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 58: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 59: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 60: Bought -0.00 shares at -0.03, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 61: Bought -0.00 shares at -0.68, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 62: Bought -0.00 shares at -0.04, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 63: Bought 0.00 shares at 0.08, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 64: Bought 0.00 shares at 0.02, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 65: Bought -0.00 shares at -0.89, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 66: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 67: Bought -0.00 shares at -0.07, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 68: Bought -0.00 shares at -0.37, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 69: Bought -0.00 shares at -0.37, Cash: 0.00, Stock: -75989.74\n",
      "BUY at step 70: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 71: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "SELL at step 72: Sold nan shares at 0.00, Cash: nan\n",
      "SELL at step 73: Sold 0.00 shares at -0.38, Cash: nan\n",
      "SELL at step 74: Sold 0.00 shares at 0.62, Cash: nan\n",
      "SELL at step 75: Sold 0.00 shares at 0.62, Cash: nan\n",
      "SELL at step 181: Sold 0.00 shares at 0.01, Cash: nan\n",
      "SELL at step 182: Sold 0.00 shares at 0.06, Cash: nan\n",
      "BUY at step 183: Bought nan shares at -0.00, Cash: nan, Stock: nan\n",
      "BUY at step 184: Bought nan shares at -0.07, Cash: nan, Stock: nan\n",
      "BUY at step 185: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 186: Bought nan shares at 0.06, Cash: nan, Stock: nan\n",
      "BUY at step 187: Bought nan shares at -0.00, Cash: nan, Stock: nan\n",
      "BUY at step 188: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 189: Bought nan shares at 0.02, Cash: nan, Stock: nan\n",
      "BUY at step 190: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 191: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 192: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 193: Bought nan shares at 0.02, Cash: nan, Stock: nan\n",
      "BUY at step 194: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 195: Bought nan shares at 0.02, Cash: nan, Stock: nan\n",
      "BUY at step 196: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 197: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 198: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 199: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 200: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 201: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 202: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 203: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 204: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 205: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 206: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 207: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 208: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 209: Bought nan shares at -0.00, Cash: nan, Stock: nan\n",
      "BUY at step 210: Bought nan shares at 0.05, Cash: nan, Stock: nan\n",
      "BUY at step 211: Bought nan shares at -0.00, Cash: nan, Stock: nan\n",
      "SELL at step 212: Sold nan shares at -0.06, Cash: nan\n",
      "SELL at step 213: Sold 0.00 shares at 0.00, Cash: nan\n",
      "SELL at step 214: Sold 0.00 shares at 0.05, Cash: nan\n",
      "SELL at step 215: Sold 0.00 shares at -0.00, Cash: nan\n",
      "SELL at step 216: Sold 0.00 shares at -0.07, Cash: nan\n",
      "SELL at step 217: Sold 0.00 shares at -0.36, Cash: nan\n",
      "SELL at step 218: Sold 0.00 shares at -0.36, Cash: nan\n",
      "SELL at step 321: Sold 0.00 shares at -1.05, Cash: nan\n",
      "SELL at step 322: Sold 0.00 shares at -1.01, Cash: nan\n",
      "SELL at step 323: Sold 0.00 shares at 0.95, Cash: nan\n",
      "SELL at step 324: Sold 0.00 shares at 0.38, Cash: nan\n",
      "SELL at step 325: Sold 0.00 shares at -0.05, Cash: nan\n",
      "SELL at step 326: Sold 0.00 shares at -0.00, Cash: nan\n",
      "SELL at step 327: Sold 0.00 shares at 2.36, Cash: nan\n",
      "SELL at step 328: Sold 0.00 shares at -1.23, Cash: nan\n",
      "SELL at step 329: Sold 0.00 shares at -0.06, Cash: nan\n",
      "BUY at step 330: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 331: Bought nan shares at 0.06, Cash: nan, Stock: nan\n",
      "BUY at step 332: Bought nan shares at -0.18, Cash: nan, Stock: nan\n",
      "BUY at step 333: Bought nan shares at -0.07, Cash: nan, Stock: nan\n",
      "BUY at step 334: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 335: Bought nan shares at 2.39, Cash: nan, Stock: nan\n",
      "BUY at step 336: Bought nan shares at -0.54, Cash: nan, Stock: nan\n",
      "BUY at step 337: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 338: Bought nan shares at 0.02, Cash: nan, Stock: nan\n",
      "BUY at step 339: Bought nan shares at 8.29, Cash: nan, Stock: nan\n",
      "BUY at step 340: Bought nan shares at -1.28, Cash: nan, Stock: nan\n",
      "BUY at step 341: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 342: Bought nan shares at 0.02, Cash: nan, Stock: nan\n",
      "BUY at step 343: Bought nan shares at 1.76, Cash: nan, Stock: nan\n",
      "BUY at step 344: Bought nan shares at 1.47, Cash: nan, Stock: nan\n",
      "BUY at step 345: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 346: Bought nan shares at 0.01, Cash: nan, Stock: nan\n",
      "BUY at step 347: Bought nan shares at 1.42, Cash: nan, Stock: nan\n",
      "BUY at step 348: Bought nan shares at -0.30, Cash: nan, Stock: nan\n",
      "BUY at step 349: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 350: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 351: Bought nan shares at 1.19, Cash: nan, Stock: nan\n",
      "BUY at step 352: Bought nan shares at -0.30, Cash: nan, Stock: nan\n",
      "BUY at step 353: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 354: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 355: Bought nan shares at 1.02, Cash: nan, Stock: nan\n",
      "BUY at step 356: Bought nan shares at -0.12, Cash: nan, Stock: nan\n",
      "BUY at step 357: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 358: Bought nan shares at -0.00, Cash: nan, Stock: nan\n",
      "BUY at step 359: Bought nan shares at 1.84, Cash: nan, Stock: nan\n",
      "BUY at step 360: Bought nan shares at 0.61, Cash: nan, Stock: nan\n",
      "BUY at step 361: Bought nan shares at -0.06, Cash: nan, Stock: nan\n",
      "BUY at step 362: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 363: Bought nan shares at 0.90, Cash: nan, Stock: nan\n",
      "BUY at step 364: Bought nan shares at -0.41, Cash: nan, Stock: nan\n",
      "BUY at step 365: Bought nan shares at -0.07, Cash: nan, Stock: nan\n",
      "BUY at step 366: Bought nan shares at 0.03, Cash: nan, Stock: nan\n",
      "BUY at step 367: Bought nan shares at 0.04, Cash: nan, Stock: nan\n",
      "BUY at step 368: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 369: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 370: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 371: Bought nan shares at -0.36, Cash: nan, Stock: nan\n",
      "BUY at step 372: Bought nan shares at 0.61, Cash: nan, Stock: nan\n",
      "BUY at step 373: Bought nan shares at 0.61, Cash: nan, Stock: nan\n",
      "BUY at step 374: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 375: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 376: Bought nan shares at 0.00, Cash: nan, Stock: nan\n",
      "BUY at step 377: Bought nan shares at -0.14, Cash: nan, Stock: nan\n",
      "BUY at step 378: Bought nan shares at 2.77, Cash: nan, Stock: nan\n",
      "BUY at step 379: Bought nan shares at 2.75, Cash: nan, Stock: nan\n",
      "Final Portfolio Value: nan, Remaining Cash: nan, Remaining Stock: nan\n",
      "\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Label distribution in training set: {0: 64, 1: 12, 2: 3}\n",
      "Label distribution in test set: {0: 378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monam\\AppData\\Local\\Temp\\ipykernel_14796\\1363679894.py:124: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  num_shares = cash / (price * (1 + transaction_cost))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9757\n",
      "Epoch 2, Loss: 0.8929\n",
      "Epoch 3, Loss: 0.9054\n",
      "Epoch 4, Loss: 0.8724\n",
      "Epoch 5, Loss: 1.0063\n",
      "Epoch 6, Loss: 0.8403\n",
      "Epoch 7, Loss: 0.8842\n",
      "Epoch 8, Loss: 0.9297\n",
      "Epoch 9, Loss: 0.8517\n",
      "Epoch 10, Loss: 0.8702\n",
      "Predictions: [1 1 1 1 1 1 1 1 1 1] ... [1 1 1 1 1 1 1 1 1 1] (first 10 and last 10 predictions)\n",
      "Initial cash: 10000, transaction cost: 0.001\n",
      "SELL at step 18: Sold 0.00 shares at -0.00, Cash: 10000.00\n",
      "SELL at step 19: Sold 0.00 shares at 3.36, Cash: 10000.00\n",
      "SELL at step 20: Sold 0.00 shares at -0.13, Cash: 10000.00\n",
      "SELL at step 21: Sold 0.00 shares at -0.06, Cash: 10000.00\n",
      "SELL at step 22: Sold 0.00 shares at 0.01, Cash: 10000.00\n",
      "SELL at step 23: Sold 0.00 shares at 1.50, Cash: 10000.00\n",
      "SELL at step 24: Sold 0.00 shares at -1.05, Cash: 10000.00\n",
      "BUY at step 25: Bought -160104.99 shares at -0.06, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 26: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 27: Bought 0.00 shares at 2.53, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 28: Bought 0.00 shares at 0.46, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 29: Bought 0.00 shares at 0.08, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 30: Bought 0.00 shares at 0.06, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 31: Bought 0.00 shares at 1.14, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 32: Bought -0.00 shares at -1.26, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 33: Bought 0.00 shares at 0.09, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 34: Bought -0.00 shares at -0.02, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 35: Bought 0.00 shares at 2.33, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 36: Bought 0.00 shares at 1.80, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 37: Bought 0.00 shares at 0.16, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 38: Bought 0.00 shares at 0.03, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 39: Bought -0.00 shares at -0.33, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 40: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 41: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 42: Bought 0.00 shares at 0.03, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 43: Bought -0.00 shares at -0.56, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 44: Bought -0.00 shares at -0.04, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 45: Bought 0.00 shares at 0.10, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 46: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 47: Bought -0.00 shares at -0.74, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 48: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 49: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 50: Bought -0.00 shares at -0.03, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 51: Bought -0.00 shares at -0.68, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 52: Bought -0.00 shares at -0.04, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 53: Bought 0.00 shares at 0.08, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 54: Bought 0.00 shares at 0.02, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 55: Bought -0.00 shares at -0.89, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 56: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 57: Bought -0.00 shares at -0.07, Cash: 0.00, Stock: -160104.99\n",
      "BUY at step 58: Bought -0.00 shares at -0.37, Cash: 0.00, Stock: -160104.99\n",
      "SELL at step 59: Sold -160104.99 shares at -0.37, Cash: 58464.28\n",
      "SELL at step 60: Sold 0.00 shares at 0.00, Cash: 58464.28\n",
      "SELL at step 61: Sold 0.00 shares at 0.00, Cash: 58464.28\n",
      "SELL at step 62: Sold 0.00 shares at 0.00, Cash: 58464.28\n",
      "SELL at step 63: Sold 0.00 shares at -0.38, Cash: 58464.28\n",
      "SELL at step 64: Sold 0.00 shares at 0.62, Cash: 58464.28\n",
      "SELL at step 65: Sold 0.00 shares at 0.62, Cash: 58464.28\n",
      "SELL at step 177: Sold 0.00 shares at -0.00, Cash: 58464.28\n",
      "SELL at step 178: Sold 0.00 shares at -0.06, Cash: 58464.28\n",
      "SELL at step 179: Sold 0.00 shares at 0.02, Cash: 58464.28\n",
      "SELL at step 180: Sold 0.00 shares at 0.05, Cash: 58464.28\n",
      "SELL at step 181: Sold 0.00 shares at 0.01, Cash: 58464.28\n",
      "SELL at step 182: Sold 0.00 shares at -0.06, Cash: 58464.28\n",
      "BUY at step 183: Bought 2737735.54 shares at 0.02, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 184: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 185: Bought 0.00 shares at 0.02, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 186: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 187: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 188: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 189: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 190: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 191: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 192: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 193: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 194: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 195: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 196: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 197: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 198: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 199: Bought -0.00 shares at -0.00, Cash: 0.00, Stock: 2737735.54\n",
      "BUY at step 200: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 2737735.54\n",
      "SELL at step 201: Sold 2737735.54 shares at -0.00, Cash: -13189.25\n",
      "SELL at step 202: Sold 0.00 shares at -0.06, Cash: -13189.25\n",
      "SELL at step 203: Sold 0.00 shares at 0.00, Cash: -13189.25\n",
      "SELL at step 204: Sold 0.00 shares at 0.05, Cash: -13189.25\n",
      "SELL at step 318: Sold 0.00 shares at -1.23, Cash: -13189.25\n",
      "SELL at step 319: Sold 0.00 shares at -0.06, Cash: -13189.25\n",
      "SELL at step 320: Sold 0.00 shares at 0.01, Cash: -13189.25\n",
      "SELL at step 321: Sold 0.00 shares at 0.06, Cash: -13189.25\n",
      "SELL at step 322: Sold 0.00 shares at -0.18, Cash: -13189.25\n",
      "SELL at step 323: Sold 0.00 shares at -0.07, Cash: -13189.25\n",
      "SELL at step 324: Sold 0.00 shares at 0.01, Cash: -13189.25\n",
      "BUY at step 325: Bought -5516.29 shares at 2.39, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 326: Bought -0.00 shares at -0.54, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 327: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 328: Bought 0.00 shares at 0.02, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 329: Bought 0.00 shares at 8.29, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 330: Bought -0.00 shares at -1.28, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 331: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 332: Bought 0.00 shares at 0.02, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 333: Bought 0.00 shares at 1.76, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 334: Bought 0.00 shares at 1.47, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 335: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 336: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 337: Bought 0.00 shares at 1.42, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 338: Bought -0.00 shares at -0.30, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 339: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 340: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 341: Bought 0.00 shares at 1.19, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 342: Bought -0.00 shares at -0.30, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 343: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 344: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 345: Bought 0.00 shares at 1.02, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 346: Bought -0.00 shares at -0.12, Cash: 0.00, Stock: -5516.29\n",
      "BUY at step 347: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: -5516.29\n",
      "SELL at step 348: Sold -5516.29 shares at -0.00, Cash: 8.52\n",
      "SELL at step 349: Sold 0.00 shares at 1.84, Cash: 8.52\n",
      "Final Portfolio Value: 8.52, Remaining Cash: 8.52, Remaining Stock: 0.00\n",
      "\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Label distribution in training set: {0: 48, 1: 8, 2: 3}\n",
      "Label distribution in test set: {0: 358}\n",
      "Epoch 1, Loss: 1.3627\n",
      "Epoch 2, Loss: 1.3532\n",
      "Epoch 3, Loss: 1.0372\n",
      "Epoch 4, Loss: 1.0670\n",
      "Epoch 5, Loss: 1.0576\n",
      "Epoch 6, Loss: 0.9789\n",
      "Epoch 7, Loss: 0.9332\n",
      "Epoch 8, Loss: 0.9406\n",
      "Epoch 9, Loss: 0.8381\n",
      "Epoch 10, Loss: 0.9589\n",
      "Predictions: [1 1 1 1 1 1 1 1 1 1] ... [1 1 1 1 1 1 1 1 1 1] (first 10 and last 10 predictions)\n",
      "Initial cash: 10000, transaction cost: 0.001\n",
      "SELL at step 19: Sold 0.00 shares at -0.33, Cash: 10000.00\n",
      "BUY at step 20: Bought 206934.10 shares at 0.05, Cash: -0.00, Stock: 206934.10\n",
      "SELL at step 21: Sold 206934.10 shares at 0.00, Cash: 408.16\n",
      "SELL at step 165: Sold 0.00 shares at 0.02, Cash: 408.16\n",
      "SELL at step 166: Sold 0.00 shares at -0.06, Cash: 408.16\n",
      "SELL at step 167: Sold 0.00 shares at 0.01, Cash: 408.16\n",
      "SELL at step 168: Sold 0.00 shares at 0.05, Cash: 408.16\n",
      "BUY at step 169: Bought 40974.40 shares at 0.01, Cash: 0.00, Stock: 40974.40\n",
      "BUY at step 170: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 40974.40\n",
      "BUY at step 171: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 40974.40\n",
      "BUY at step 172: Bought 0.00 shares at 0.05, Cash: 0.00, Stock: 40974.40\n",
      "SELL at step 173: Sold 40974.40 shares at 0.00, Cash: 200.35\n",
      "SELL at step 174: Sold 0.00 shares at -0.06, Cash: 200.35\n",
      "SELL at step 175: Sold 0.00 shares at 0.00, Cash: 200.35\n",
      "SELL at step 176: Sold 0.00 shares at 0.05, Cash: 200.35\n",
      "SELL at step 177: Sold 0.00 shares at 0.00, Cash: 200.35\n",
      "SELL at step 178: Sold 0.00 shares at -0.06, Cash: 200.35\n",
      "SELL at step 179: Sold 0.00 shares at -0.00, Cash: 200.35\n",
      "SELL at step 180: Sold 0.00 shares at 0.05, Cash: 200.35\n",
      "SELL at step 308: Sold 0.00 shares at 0.02, Cash: 200.35\n",
      "SELL at step 309: Sold 0.00 shares at 8.29, Cash: 200.35\n",
      "SELL at step 310: Sold 0.00 shares at -1.28, Cash: 200.35\n",
      "SELL at step 311: Sold 0.00 shares at -0.06, Cash: 200.35\n",
      "SELL at step 312: Sold 0.00 shares at 0.02, Cash: 200.35\n",
      "BUY at step 313: Bought 113.51 shares at 1.76, Cash: -0.00, Stock: 113.51\n",
      "BUY at step 314: Bought -0.00 shares at 1.47, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 315: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 316: Bought 0.00 shares at 0.01, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 317: Bought 0.00 shares at 1.42, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 318: Bought -0.00 shares at -0.30, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 319: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 320: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 321: Bought 0.00 shares at 1.19, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 322: Bought -0.00 shares at -0.30, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 323: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 324: Bought 0.00 shares at 0.00, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 325: Bought 0.00 shares at 1.02, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 326: Bought -0.00 shares at -0.12, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 327: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 328: Bought -0.00 shares at -0.00, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 329: Bought 0.00 shares at 1.84, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 330: Bought 0.00 shares at 0.61, Cash: 0.00, Stock: 113.51\n",
      "BUY at step 331: Bought -0.00 shares at -0.06, Cash: 0.00, Stock: 113.51\n",
      "SELL at step 332: Sold 113.51 shares at 0.00, Cash: 0.22\n",
      "SELL at step 333: Sold 0.00 shares at 0.90, Cash: 0.22\n",
      "SELL at step 334: Sold 0.00 shares at -0.41, Cash: 0.22\n",
      "SELL at step 335: Sold 0.00 shares at -0.07, Cash: 0.22\n",
      "SELL at step 336: Sold 0.00 shares at 0.03, Cash: 0.22\n",
      "SELL at step 337: Sold 0.00 shares at 0.04, Cash: 0.22\n",
      "SELL at step 338: Sold 0.00 shares at 0.00, Cash: 0.22\n",
      "SELL at step 339: Sold 0.00 shares at 0.00, Cash: 0.22\n",
      "Final Portfolio Value: 0.22, Remaining Cash: 0.22, Remaining Stock: 0.00\n",
      "\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Label distribution in training set: {0: 5, 1: 5}\n",
      "Label distribution in test set: {0: 308}\n",
      "Epoch 1, Loss: 1.1544\n",
      "Epoch 2, Loss: 1.0726\n",
      "Epoch 3, Loss: 1.0053\n",
      "Epoch 4, Loss: 0.9597\n",
      "Epoch 5, Loss: 0.9105\n",
      "Epoch 6, Loss: 0.8939\n",
      "Epoch 7, Loss: 0.8514\n",
      "Epoch 8, Loss: 0.8817\n",
      "Epoch 9, Loss: 0.8583\n",
      "Epoch 10, Loss: 0.8666\n",
      "Predictions: [0 0 1 1 1 1 1 1 1 1] ... [0 0 1 1 1 1 1 1 1 1] (first 10 and last 10 predictions)\n",
      "Initial cash: 10000, transaction cost: 0.001\n",
      "SELL at step 0: Sold 0.00 shares at 0.15, Cash: 10000.00\n",
      "SELL at step 1: Sold 0.00 shares at 0.14, Cash: 10000.00\n",
      "SELL at step 149: Sold 0.00 shares at -0.83, Cash: 10000.00\n",
      "SELL at step 150: Sold 0.00 shares at -0.84, Cash: 10000.00\n",
      "SELL at step 151: Sold 0.00 shares at 0.00, Cash: 10000.00\n",
      "SELL at step 152: Sold 0.00 shares at 0.00, Cash: 10000.00\n",
      "SELL at step 153: Sold 0.00 shares at 0.00, Cash: 10000.00\n",
      "SELL at step 168: Sold 0.00 shares at -0.87, Cash: 10000.00\n",
      "SELL at step 169: Sold 0.00 shares at 0.38, Cash: 10000.00\n",
      "SELL at step 170: Sold 0.00 shares at -0.68, Cash: 10000.00\n",
      "SELL at step 171: Sold 0.00 shares at 0.38, Cash: 10000.00\n",
      "SELL at step 225: Sold 0.00 shares at 1.51, Cash: 10000.00\n",
      "SELL at step 226: Sold 0.00 shares at -0.41, Cash: 10000.00\n",
      "SELL at step 227: Sold 0.00 shares at 1.37, Cash: 10000.00\n",
      "SELL at step 228: Sold 0.00 shares at 1.09, Cash: 10000.00\n",
      "SELL at step 229: Sold 0.00 shares at -0.30, Cash: 10000.00\n",
      "SELL at step 230: Sold 0.00 shares at -0.33, Cash: 10000.00\n",
      "BUY at step 231: Bought 2672.21 shares at 3.74, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 232: Bought -0.00 shares at -0.25, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 233: Bought -0.00 shares at -0.28, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 234: Bought -0.00 shares at -0.32, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 235: Bought -0.00 shares at -0.37, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 236: Bought 0.00 shares at 0.12, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 237: Bought -0.00 shares at -0.42, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 238: Bought 0.00 shares at 0.27, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 239: Bought 0.00 shares at 0.38, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 240: Bought 0.00 shares at 0.38, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 241: Bought -0.00 shares at -1.05, Cash: 0.00, Stock: 2672.21\n",
      "BUY at step 242: Bought -0.00 shares at -1.01, Cash: 0.00, Stock: 2672.21\n",
      "SELL at step 243: Sold 2672.21 shares at 0.95, Cash: 2540.20\n",
      "SELL at step 244: Sold 0.00 shares at 0.38, Cash: 2540.20\n",
      "SELL at step 245: Sold 0.00 shares at -0.05, Cash: 2540.20\n",
      "SELL at step 246: Sold 0.00 shares at -0.00, Cash: 2540.20\n",
      "SELL at step 247: Sold 0.00 shares at 2.36, Cash: 2540.20\n",
      "SELL at step 248: Sold 0.00 shares at -1.23, Cash: 2540.20\n",
      "SELL at step 249: Sold 0.00 shares at -0.06, Cash: 2540.20\n",
      "SELL at step 250: Sold 0.00 shares at 0.01, Cash: 2540.20\n",
      "SELL at step 251: Sold 0.00 shares at 0.06, Cash: 2540.20\n",
      "SELL at step 252: Sold 0.00 shares at -0.18, Cash: 2540.20\n",
      "SELL at step 253: Sold 0.00 shares at -0.07, Cash: 2540.20\n",
      "SELL at step 254: Sold 0.00 shares at 0.01, Cash: 2540.20\n",
      "SELL at step 298: Sold 0.00 shares at 2.77, Cash: 2540.20\n",
      "SELL at step 299: Sold 0.00 shares at 2.75, Cash: 2540.20\n",
      "Final Portfolio Value: 2540.20, Remaining Cash: 2540.20, Remaining Stock: 0.00\n",
      "Results:\n",
      "   Horizon  Portfolio Value\n",
      "0       10     10000.000000\n",
      "1       20              NaN\n",
      "2       30         8.524953\n",
      "3       50         0.219930\n",
      "4      100      2540.201425\n"
     ]
    }
   ],
   "source": [
    "# 10 Back testing on F1 dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return np.loadtxt(file_path)\n",
    "\n",
    "dec_train = load_data(train_file)\n",
    "dec_test1 = load_data(test_file1)\n",
    "dec_test2 = load_data(test_file2)\n",
    "dec_test3 = load_data(test_file3)\n",
    "\n",
    "# Preprocess to handle mismatched dimensions\n",
    "def preprocess_data(*arrays):\n",
    "    max_cols = max(array.shape[1] for array in arrays)\n",
    "    processed_arrays = []\n",
    "    for array in arrays:\n",
    "        if array.shape[1] < max_cols:\n",
    "            padded = np.pad(array, ((0, 0), (0, max_cols - array.shape[1])), mode='constant', constant_values=0)\n",
    "            processed_arrays.append(padded)\n",
    "        elif array.shape[1] > max_cols:\n",
    "            processed_arrays.append(array[:, :max_cols])\n",
    "        else:\n",
    "            processed_arrays.append(array)\n",
    "    return processed_arrays\n",
    "\n",
    "dec_train, dec_test1, dec_test2, dec_test3 = preprocess_data(dec_train, dec_test1, dec_test2, dec_test3)\n",
    "\n",
    "# Combine test data\n",
    "test_data_combined = np.vstack([dec_test1, dec_test2, dec_test3])\n",
    "\n",
    "# Parameters\n",
    "W, dim, num_classes, batch_size = 40, 40, 3, 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon, W, num_classes):\n",
    "    sequences, labels, prices = [], [], []\n",
    "    N = data.shape[0]\n",
    "    for i in range(N - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :W]\n",
    "        label = int(data[i + dim + horizon - 1, -1])  # Labels: 0 (Sell), 1 (Hold), 2 (Buy)\n",
    "        price = data[i + dim + horizon - 1, 0]  # Assuming first column is price\n",
    "        if 0 <= label < num_classes:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "            prices.append(price)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64), np.array(prices)\n",
    "\n",
    "# Check label distribution\n",
    "def print_label_distribution(y, dataset_name):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    label_distribution = dict(zip(unique, counts))\n",
    "    print(f\"Label distribution in {dataset_name}: {label_distribution}\")\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# AxialLOB Model\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, c_in, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(c_in)\n",
    "        self.conv2 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "        self.conv3 = nn.Conv2d(c_out, c_final, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(c_final)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c_final, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Handle data imbalance using class weights\n",
    "def compute_class_weights(y):\n",
    "    class_sample_count = np.array([len(np.where(y == t)[0]) for t in np.unique(y)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in y])\n",
    "    return torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Trading Strategy Simulation\n",
    "def simulate_trading_debug(preds, prices, initial_cash=10000, transaction_cost=0.001):\n",
    "    cash = initial_cash\n",
    "    stock = 0\n",
    "    print(f\"Initial cash: {cash}, transaction cost: {transaction_cost}\")\n",
    "\n",
    "    for i, (pred, price) in enumerate(zip(preds, prices)):\n",
    "        if pred == 2:  # Buy signal\n",
    "            num_shares = cash / (price * (1 + transaction_cost))\n",
    "            cash -= num_shares * price * (1 + transaction_cost)\n",
    "            stock += num_shares\n",
    "            print(f\"BUY at step {i}: Bought {num_shares:.2f} shares at {price:.2f}, Cash: {cash:.2f}, Stock: {stock:.2f}\")\n",
    "        elif pred == 0:  # Sell signal\n",
    "            cash += stock * price * (1 - transaction_cost)\n",
    "            print(f\"SELL at step {i}: Sold {stock:.2f} shares at {price:.2f}, Cash: {cash:.2f}\")\n",
    "            stock = 0\n",
    "        # Hold signal does nothing\n",
    "\n",
    "    portfolio_value = cash + stock * prices[-1]\n",
    "    print(f\"Final Portfolio Value: {portfolio_value:.2f}, Remaining Cash: {cash:.2f}, Remaining Stock: {stock:.2f}\")\n",
    "    return portfolio_value\n",
    "\n",
    "# Evaluation with Debugging\n",
    "def evaluate_strategy_debug(model, train_data, test_data, horizons, batch_size=32):\n",
    "    model.to(device)\n",
    "    results = []\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\nEvaluating for Prediction Horizon: {horizon}\")\n",
    "        X_train, y_train, _ = create_sequences(train_data, dim, horizon, W, num_classes)\n",
    "        X_test, y_test, prices_test = create_sequences(test_data, dim, horizon, W, num_classes)\n",
    "\n",
    "        # Print label distribution\n",
    "        print_label_distribution(y_train, \"training set\")\n",
    "        print_label_distribution(y_test, \"test set\")\n",
    "\n",
    "        # Handle data imbalance\n",
    "        class_weights = compute_class_weights(y_train)\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "        train_dataset = Dataset(X_train, y_train)\n",
    "        train_sampler = data.WeightedRandomSampler(class_weights, len(class_weights))\n",
    "\n",
    "        train_loader = data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = data.DataLoader(Dataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(10):  # Increased epochs\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(all_preds)\n",
    "        print(f\"Predictions: {preds[:10]} ... {preds[-10:]} (first 10 and last 10 predictions)\")\n",
    "        portfolio_value = simulate_trading_debug(preds, prices_test)\n",
    "        results.append({\"Horizon\": horizon, \"Portfolio Value\": portfolio_value})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Instantiate model\n",
    "model = AxialLOB(W, dim, 16, 16, 16, num_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_strategy_debug(model, dec_train, test_data_combined, [10, 20, 30, 50, 100])\n",
    "print(f\"Results:\\n{results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Fetched 737 rows of data.\n",
      "\n",
      "Evaluating AxialLOB\n",
      "\n",
      "Evaluating for Prediction Horizon: 5\n",
      "Epoch 1, Loss: 1.0069\n",
      "Epoch 2, Loss: 0.8917\n",
      "Epoch 3, Loss: 0.8635\n",
      "Epoch 4, Loss: 0.8645\n",
      "Epoch 5, Loss: 0.8643\n",
      "Epoch 6, Loss: 0.8829\n",
      "Epoch 7, Loss: 0.8851\n",
      "Epoch 8, Loss: 0.8610\n",
      "Epoch 9, Loss: 0.8562\n",
      "Epoch 10, Loss: 0.8541\n",
      "\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Epoch 1, Loss: 0.8634\n",
      "Epoch 2, Loss: 0.8631\n",
      "Epoch 3, Loss: 0.8598\n",
      "Epoch 4, Loss: 0.8658\n",
      "Epoch 5, Loss: 0.8544\n",
      "Epoch 6, Loss: 0.8531\n",
      "Epoch 7, Loss: 0.8505\n",
      "Epoch 8, Loss: 0.8545\n",
      "Epoch 9, Loss: 0.8547\n",
      "Epoch 10, Loss: 0.8539\n",
      "\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Epoch 1, Loss: 0.8700\n",
      "Epoch 2, Loss: 0.8618\n",
      "Epoch 3, Loss: 0.8582\n",
      "Epoch 4, Loss: 0.8634\n",
      "Epoch 5, Loss: 0.8630\n",
      "Epoch 6, Loss: 0.8585\n",
      "Epoch 7, Loss: 0.8618\n",
      "Epoch 8, Loss: 0.8603\n",
      "Epoch 9, Loss: 0.8569\n",
      "Epoch 10, Loss: 0.8552\n",
      "\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Epoch 1, Loss: 0.8644\n",
      "Epoch 2, Loss: 0.8767\n",
      "Epoch 3, Loss: 0.8590\n",
      "Epoch 4, Loss: 0.8651\n",
      "Epoch 5, Loss: 0.8609\n",
      "Epoch 6, Loss: 0.8595\n",
      "Epoch 7, Loss: 0.8618\n",
      "Epoch 8, Loss: 0.8639\n",
      "Epoch 9, Loss: 0.8675\n",
      "Epoch 10, Loss: 0.8612\n",
      "\n",
      "Evaluating for Prediction Horizon: 40\n",
      "Epoch 1, Loss: 0.8810\n",
      "Epoch 2, Loss: 0.8815\n",
      "Epoch 3, Loss: 0.8592\n",
      "Epoch 4, Loss: 0.8738\n",
      "Epoch 5, Loss: 0.8576\n",
      "Epoch 6, Loss: 0.8589\n",
      "Epoch 7, Loss: 0.8500\n",
      "Epoch 8, Loss: 0.8479\n",
      "Epoch 9, Loss: 0.8602\n",
      "Epoch 10, Loss: 0.8448\n",
      "\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Epoch 1, Loss: 0.8626\n",
      "Epoch 2, Loss: 0.9064\n",
      "Epoch 3, Loss: 0.8528\n",
      "Epoch 4, Loss: 0.8558\n",
      "Epoch 5, Loss: 0.8530\n",
      "Epoch 6, Loss: 0.8430\n",
      "Epoch 7, Loss: 0.8501\n",
      "Epoch 8, Loss: 0.9109\n",
      "Epoch 9, Loss: 0.8408\n",
      "Epoch 10, Loss: 0.8475\n",
      "\n",
      "Evaluating for Prediction Horizon: 60\n",
      "Epoch 1, Loss: 0.8757\n",
      "Epoch 2, Loss: 0.8562\n",
      "Epoch 3, Loss: 0.8525\n",
      "Epoch 4, Loss: 0.8511\n",
      "Epoch 5, Loss: 0.8651\n",
      "Epoch 6, Loss: 0.8522\n",
      "Epoch 7, Loss: 0.8452\n",
      "Epoch 8, Loss: 0.8474\n",
      "Epoch 9, Loss: 0.8472\n",
      "Epoch 10, Loss: 0.8407\n",
      "\n",
      "Evaluating for Prediction Horizon: 70\n",
      "Epoch 1, Loss: 0.8797\n",
      "Epoch 2, Loss: 0.8736\n",
      "Epoch 3, Loss: 0.8571\n",
      "Epoch 4, Loss: 0.8605\n",
      "Epoch 5, Loss: 0.8593\n",
      "Epoch 6, Loss: 0.8628\n",
      "Epoch 7, Loss: 0.8602\n",
      "Epoch 8, Loss: 0.8488\n",
      "Epoch 9, Loss: 0.8467\n",
      "Epoch 10, Loss: 0.8474\n",
      "\n",
      "Evaluating for Prediction Horizon: 80\n",
      "Epoch 1, Loss: 0.8720\n",
      "Epoch 2, Loss: 0.8598\n",
      "Epoch 3, Loss: 0.8711\n",
      "Epoch 4, Loss: 0.8580\n",
      "Epoch 5, Loss: 0.8565\n",
      "Epoch 6, Loss: 0.8458\n",
      "Epoch 7, Loss: 0.8631\n",
      "Epoch 8, Loss: 0.8601\n",
      "Epoch 9, Loss: 0.8621\n",
      "Epoch 10, Loss: 0.8580\n",
      "\n",
      "Evaluating for Prediction Horizon: 90\n",
      "Epoch 1, Loss: 0.8661\n",
      "Epoch 2, Loss: 0.8652\n",
      "Epoch 3, Loss: 0.8519\n",
      "Epoch 4, Loss: 0.8495\n",
      "Epoch 5, Loss: 0.8495\n",
      "Epoch 6, Loss: 0.8492\n",
      "Epoch 7, Loss: 0.8574\n",
      "Epoch 8, Loss: 0.8457\n",
      "Epoch 9, Loss: 0.8502\n",
      "Epoch 10, Loss: 0.8408\n",
      "\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Epoch 1, Loss: 0.8730\n",
      "Epoch 2, Loss: 0.8647\n",
      "Epoch 3, Loss: 0.8623\n",
      "Epoch 4, Loss: 0.8609\n",
      "Epoch 5, Loss: 0.8529\n",
      "Epoch 6, Loss: 0.8542\n",
      "Epoch 7, Loss: 0.8527\n",
      "Epoch 8, Loss: 0.8536\n",
      "Epoch 9, Loss: 0.8476\n",
      "Epoch 10, Loss: 0.8445\n",
      "Results for AxialLOB:\n",
      "    Horizon  Portfolio Value\n",
      "0         5      8928.338707\n",
      "1        10      8881.907282\n",
      "2        20      8707.523620\n",
      "3        30      9465.484421\n",
      "4        40      9777.056181\n",
      "5        50     10165.426068\n",
      "6        60     11485.678598\n",
      "7        70     11092.710886\n",
      "8        80      8877.615586\n",
      "9        90      8800.966902\n",
      "10      100      9862.175320\n",
      "\n",
      "Evaluating DeepLOB\n",
      "\n",
      "Evaluating for Prediction Horizon: 5\n",
      "Epoch 1, Loss: 0.9167\n",
      "Epoch 2, Loss: 0.8565\n",
      "Epoch 3, Loss: 0.8688\n",
      "Epoch 4, Loss: 0.8652\n",
      "Epoch 5, Loss: 0.8894\n",
      "Epoch 6, Loss: 0.8739\n",
      "Epoch 7, Loss: 0.8698\n",
      "Epoch 8, Loss: 0.8652\n",
      "Epoch 9, Loss: 0.8683\n",
      "Epoch 10, Loss: 0.8777\n",
      "\n",
      "Evaluating for Prediction Horizon: 10\n",
      "Epoch 1, Loss: 0.8703\n",
      "Epoch 2, Loss: 0.8669\n",
      "Epoch 3, Loss: 0.8610\n",
      "Epoch 4, Loss: 0.8666\n",
      "Epoch 5, Loss: 0.8556\n",
      "Epoch 6, Loss: 0.8625\n",
      "Epoch 7, Loss: 0.8597\n",
      "Epoch 8, Loss: 0.8497\n",
      "Epoch 9, Loss: 0.8598\n",
      "Epoch 10, Loss: 0.8566\n",
      "\n",
      "Evaluating for Prediction Horizon: 20\n",
      "Epoch 1, Loss: 0.8706\n",
      "Epoch 2, Loss: 0.8653\n",
      "Epoch 3, Loss: 0.8636\n",
      "Epoch 4, Loss: 0.8718\n",
      "Epoch 5, Loss: 0.8612\n",
      "Epoch 6, Loss: 0.8640\n",
      "Epoch 7, Loss: 0.8650\n",
      "Epoch 8, Loss: 0.8598\n",
      "Epoch 9, Loss: 0.8652\n",
      "Epoch 10, Loss: 0.8622\n",
      "\n",
      "Evaluating for Prediction Horizon: 30\n",
      "Epoch 1, Loss: 0.8752\n",
      "Epoch 2, Loss: 0.8670\n",
      "Epoch 3, Loss: 0.8645\n",
      "Epoch 4, Loss: 0.8617\n",
      "Epoch 5, Loss: 0.8679\n",
      "Epoch 6, Loss: 0.8589\n",
      "Epoch 7, Loss: 0.8626\n",
      "Epoch 8, Loss: 0.8626\n",
      "Epoch 9, Loss: 0.8574\n",
      "Epoch 10, Loss: 0.8677\n",
      "\n",
      "Evaluating for Prediction Horizon: 40\n",
      "Epoch 1, Loss: 0.8656\n",
      "Epoch 2, Loss: 0.8665\n",
      "Epoch 3, Loss: 0.8562\n",
      "Epoch 4, Loss: 0.8575\n",
      "Epoch 5, Loss: 0.8676\n",
      "Epoch 6, Loss: 0.8799\n",
      "Epoch 7, Loss: 0.8635\n",
      "Epoch 8, Loss: 0.8600\n",
      "Epoch 9, Loss: 0.8735\n",
      "Epoch 10, Loss: 0.8655\n",
      "\n",
      "Evaluating for Prediction Horizon: 50\n",
      "Epoch 1, Loss: 0.8778\n",
      "Epoch 2, Loss: 0.8577\n",
      "Epoch 3, Loss: 0.8719\n",
      "Epoch 4, Loss: 0.8547\n",
      "Epoch 5, Loss: 0.8637\n",
      "Epoch 6, Loss: 0.8625\n",
      "Epoch 7, Loss: 0.8547\n",
      "Epoch 8, Loss: 0.8947\n",
      "Epoch 9, Loss: 0.8583\n",
      "Epoch 10, Loss: 0.8607\n",
      "\n",
      "Evaluating for Prediction Horizon: 60\n",
      "Epoch 1, Loss: 0.8718\n",
      "Epoch 2, Loss: 0.8675\n",
      "Epoch 3, Loss: 0.8570\n",
      "Epoch 4, Loss: 0.8593\n",
      "Epoch 5, Loss: 0.8703\n",
      "Epoch 6, Loss: 0.8637\n",
      "Epoch 7, Loss: 0.8557\n",
      "Epoch 8, Loss: 0.8596\n",
      "Epoch 9, Loss: 0.8620\n",
      "Epoch 10, Loss: 0.8651\n",
      "\n",
      "Evaluating for Prediction Horizon: 70\n",
      "Epoch 1, Loss: 0.8775\n",
      "Epoch 2, Loss: 0.8739\n",
      "Epoch 3, Loss: 0.8620\n",
      "Epoch 4, Loss: 0.8642\n",
      "Epoch 5, Loss: 0.8615\n",
      "Epoch 6, Loss: 0.8615\n",
      "Epoch 7, Loss: 0.8692\n",
      "Epoch 8, Loss: 0.8661\n",
      "Epoch 9, Loss: 0.8570\n",
      "Epoch 10, Loss: 0.8671\n",
      "\n",
      "Evaluating for Prediction Horizon: 80\n",
      "Epoch 1, Loss: 0.8650\n",
      "Epoch 2, Loss: 0.8493\n",
      "Epoch 3, Loss: 0.8655\n",
      "Epoch 4, Loss: 0.8457\n",
      "Epoch 5, Loss: 0.8463\n",
      "Epoch 6, Loss: 0.8508\n",
      "Epoch 7, Loss: 0.8464\n",
      "Epoch 8, Loss: 0.8554\n",
      "Epoch 9, Loss: 0.8495\n",
      "Epoch 10, Loss: 0.8506\n",
      "\n",
      "Evaluating for Prediction Horizon: 90\n",
      "Epoch 1, Loss: 0.8814\n",
      "Epoch 2, Loss: 0.8599\n",
      "Epoch 3, Loss: 0.8606\n",
      "Epoch 4, Loss: 0.8570\n",
      "Epoch 5, Loss: 0.8548\n",
      "Epoch 6, Loss: 0.8535\n",
      "Epoch 7, Loss: 0.8350\n",
      "Epoch 8, Loss: 0.8490\n",
      "Epoch 9, Loss: 0.8472\n",
      "Epoch 10, Loss: 0.8437\n",
      "\n",
      "Evaluating for Prediction Horizon: 100\n",
      "Epoch 1, Loss: 0.8741\n",
      "Epoch 2, Loss: 0.8636\n",
      "Epoch 3, Loss: 0.8615\n",
      "Epoch 4, Loss: 0.8470\n",
      "Epoch 5, Loss: 0.8435\n",
      "Epoch 6, Loss: 0.8530\n",
      "Epoch 7, Loss: 0.8435\n",
      "Epoch 8, Loss: 0.8412\n",
      "Epoch 9, Loss: 0.8262\n",
      "Epoch 10, Loss: 0.8404\n",
      "Results for DeepLOB:\n",
      "    Horizon  Portfolio Value\n",
      "0         5      8928.338707\n",
      "1        10      8881.907282\n",
      "2        20      8472.271416\n",
      "3        30      8421.878021\n",
      "4        40     10091.017965\n",
      "5        50      9980.024213\n",
      "6        60      9553.263940\n",
      "7        70      9934.746446\n",
      "8        80     10316.517742\n",
      "9        90      9594.084073\n",
      "10      100      8327.226621\n"
     ]
    }
   ],
   "source": [
    "# 11 Backtest on AAPL data from yahoo finance\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Fetch stock data and calculate technical indicators\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    \n",
    "    # Add Returns\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Add Moving Average\n",
    "    df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    \n",
    "    # Add Exponential Moving Average\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    \n",
    "    # Add Relative Strength Index (RSI)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=14).mean()\n",
    "    df['RSI'] = 100 - (100 / (1 + gain / loss))\n",
    "    \n",
    "    # Add MACD\n",
    "    short_ema = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    long_ema = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = short_ema - long_ema\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Add Bollinger Bands\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['BB_Upper'] = df['MA_20'] + (2 * df['Close'].rolling(window=20).std())\n",
    "    df['BB_Lower'] = df['MA_20'] - (2 * df['Close'].rolling(window=20).std())\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Parameters for data fetch\n",
    "ticker = \"AAPL\"  # Example: Apple stock\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Fetch data\n",
    "data = fetch_stock_data(ticker, start_date, end_date)\n",
    "print(f\"Fetched {len(data)} rows of data.\")\n",
    "\n",
    "# Convert to numpy (include technical indicators)\n",
    "data_np = data[['Open', 'High', 'Low', 'Close', 'Volume', 'Returns', 'MA_10', 'EMA_10', 'RSI', 'MACD', 'Signal_Line', 'BB_Upper', 'BB_Lower']].to_numpy()\n",
    "\n",
    "# Parameters\n",
    "W = data_np.shape[1]  # Number of features (13 in this case)\n",
    "dim = 5               # Sequence length\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, dim, horizon):\n",
    "    sequences, labels, prices = [], [], []\n",
    "    N = data.shape[0]\n",
    "    for i in range(N - dim - horizon + 1):\n",
    "        seq = data[i:i + dim, :]\n",
    "        # Label: 2 (Buy) if return > 0.001, 0 (Sell) if return < -0.001, else 1 (Hold)\n",
    "        ret = data[i + dim + horizon - 1, 5]  # Returns column index\n",
    "        label = 2 if ret > 0.001 else (0 if ret < -0.001 else 1)\n",
    "        price = data[i + dim + horizon - 1, 3]  # Close price\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "        prices.append(price)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.int64), np.array(prices)\n",
    "\n",
    "# Dataset class\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)  # Shape: (1, dim, W)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# AxialLOB Model (Remains the same)\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, c_in, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(c_in)\n",
    "        self.conv2 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "        self.conv3 = nn.Conv2d(c_out, c_final, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(c_final)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c_final, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Corrected DeepLOB Model\n",
    "class DeepLOB(nn.Module):\n",
    "    def __init__(self, W, H, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.W = W  # Number of features\n",
    "        self.H = H  # Sequence length\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Compute LSTM input size\n",
    "        self.lstm_input_size = 128 * self.W  # channels * width\n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, H, W)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # x: (batch_size, 32, H, W)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))  # x: (batch_size, 64, H, W)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))  # x: (batch_size, 128, H, W)\n",
    "        \n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Reshape x to (batch_size, height, channels * width) for LSTM\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # x: (batch_size, height, channels, width)\n",
    "        x = x.view(batch_size, height, -1)      # x: (batch_size, height, channels * width)\n",
    "        \n",
    "        x, _ = self.lstm(x)                     # x: (batch_size, height, 2 * hidden_size)\n",
    "        x = x[:, -1, :]                         # Take the last time step\n",
    "        return self.fc(x)\n",
    "\n",
    "# Trading Strategy Simulation (Remains the same)\n",
    "def simulate_trading_debug(preds, prices, initial_cash=10000, transaction_cost=0.001):\n",
    "    cash = initial_cash\n",
    "    stock = 0\n",
    "    for i, (pred, price) in enumerate(zip(preds, prices)):\n",
    "        if price <= 0:\n",
    "            continue\n",
    "        if pred == 2:  # Buy\n",
    "            if cash > 0:\n",
    "                num_shares = cash / (price * (1 + transaction_cost))\n",
    "                cash -= num_shares * price * (1 + transaction_cost)\n",
    "                stock += num_shares\n",
    "        elif pred == 0:  # Sell\n",
    "            if stock > 0:\n",
    "                cash += stock * price * (1 - transaction_cost)\n",
    "                stock = 0\n",
    "    portfolio_value = cash + stock * prices[-1]\n",
    "    return portfolio_value\n",
    "\n",
    "# Evaluation with Debugging (Remains the same)\n",
    "def evaluate_strategy_debug(model, data_np, horizons, batch_size=32):\n",
    "    results = []\n",
    "    model.to(device)\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\nEvaluating for Prediction Horizon: {horizon}\")\n",
    "        X, y, prices = create_sequences(data_np, dim, horizon)\n",
    "        train_size = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        prices_test = prices[train_size:]\n",
    "        \n",
    "        train_loader = DataLoader(StockDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(StockDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(10):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "        preds = np.concatenate(all_preds)\n",
    "        portfolio_value = simulate_trading_debug(preds, prices_test)\n",
    "        results.append({\"Horizon\": horizon, \"Portfolio Value\": portfolio_value})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Instantiate models\n",
    "hidden_size = 64\n",
    "models = {\n",
    "    \"AxialLOB\": AxialLOB(W, dim, 16, 16, 16, num_classes),\n",
    "    \"DeepLOB\": DeepLOB(W, dim, hidden_size, num_classes),\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "horizons = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}\")\n",
    "    results = evaluate_strategy_debug(model, data_np, horizons)\n",
    "    print(f\"Results for {model_name}:\\n{results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
