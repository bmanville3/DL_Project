{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-PVsZeWjCiw"
   },
   "source": [
    "### **Axial LOB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVPONVeVw0nh",
    "outputId": "5301a18a-5c77-4648-f73a-63eca27d0f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (203800, 40)\n",
      "Validation data shape: (50950, 40)\n",
      "Testing data shape: (139587, 40)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "\n",
    "# Define paths based on your directory structure for NoAuction\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load training and validation data\n",
    "dec_data = np.loadtxt(train_file)\n",
    "dec_train = dec_data[:, :int(dec_data.shape[1] * 0.8)]\n",
    "dec_val = dec_data[:, int(dec_data.shape[1] * 0.8):]\n",
    "\n",
    "# Load test data and concatenate\n",
    "dec_test1 = np.loadtxt(test_file1)\n",
    "dec_test2 = np.loadtxt(test_file2)\n",
    "dec_test3 = np.loadtxt(test_file3)\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# Set parameters\n",
    "W = 40                     # Number of features\n",
    "dim = 40                   # Number of LOB states\n",
    "horizon = 2                # Horizon for target calculation\n",
    "T = 5                      # Time window size for dataset creation\n",
    "\n",
    "# Prepare labels\n",
    "y_train = dec_train[-horizon, :].flatten()\n",
    "y_val = dec_val[-horizon, :].flatten()\n",
    "y_test = dec_test[-horizon, :].flatten()\n",
    "\n",
    "# Adjust labels for training, validation, and test sets\n",
    "y_train = y_train[dim-1:] - 1\n",
    "y_val = y_val[dim-1:] - 1\n",
    "y_test = y_test[dim-1:] - 1 \n",
    "\n",
    "# Prepare data for model input\n",
    "dec_train = dec_train[:40, :].T\n",
    "dec_val = dec_val[:40, :].T\n",
    "dec_test = dec_test[:40, :].T\n",
    "\n",
    "# Print shapes to verify data\n",
    "print(\"Training data shape:\", dec_train.shape)\n",
    "print(\"Validation data shape:\", dec_val.shape)\n",
    "print(\"Testing data shape:\", dec_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 203756\n",
      "Dataset length: 50906\n",
      "Dataset length: 139543\n",
      "Input batch shape: torch.Size([64, 1, 40, 40])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, x, y, num_classes, dim):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.x = x   \n",
    "        self.y = y\n",
    "\n",
    "        # Compute length based on rolling window\n",
    "        self.length = x.shape[0] - T - self.dim + 1\n",
    "        print(\"Dataset length:\", self.length)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x = torch.from_numpy(x).float()  # Ensure data is float for model input\n",
    "        self.x = torch.unsqueeze(x, 1)   # Add channel dimension\n",
    "        self.y = torch.from_numpy(y).long()  # Labels should be long type for classification\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Extract input with rolling window and adjust shape\n",
    "        input = self.x[i:i+self.dim, :]\n",
    "        input = input.permute(1, 0, 2)  # Adjust to expected shape [1, dim, features]\n",
    "        \n",
    "        return input, self.y[i]\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 64\n",
    "num_classes = 3  # Adjust based on your problem (e.g., 3 classes for LOB levels)\n",
    "dim = 40  # Number of LOB states, adjust as needed\n",
    "\n",
    "# Instantiate Dataset objects for train, validation, and test sets\n",
    "dataset_train = Dataset(dec_train, y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val, y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test, y_test, num_classes, dim)\n",
    "\n",
    "# Create DataLoader objects for batching and shuffling\n",
    "train_loader = data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify DataLoader functionality\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input batch shape:\", inputs.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    break  # Test with a single batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (203759, 40)\n",
      "Training labels shape: (203759,)\n",
      "Validation data shape: (50909, 40)\n",
      "Validation labels shape: (50909,)\n",
      "Testing data shape: (139546, 40)\n",
      "Testing labels shape: (139546,)\n",
      "Dataset length: 203756\n",
      "Dataset length: 50906\n",
      "Dataset length: 139543\n",
      "Sample input batch shape: torch.Size([64, 1, 40, 40])\n",
      "Sample label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 50 \n",
    "c_final = 4              # Channel output size of the second conv layer\n",
    "n_heads = 4\n",
    "c_in_axial = 32          # Channel output size of the first conv layer\n",
    "c_out_axial = 32\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# Adjust label preparation without flattening the entire dataset\n",
    "horizon = 2\n",
    "dim = 40\n",
    "\n",
    "# Define lengths based on data size and required horizon offset\n",
    "train_len = dec_train.shape[0] - dim + 1 - horizon\n",
    "val_len = dec_val.shape[0] - dim + 1 - horizon\n",
    "test_len = dec_test.shape[0] - dim + 1 - horizon\n",
    "\n",
    "# Slicing the labels to match the data lengths exactly\n",
    "y_train = dec_train[dim-1:dim-1 + train_len, -horizon] - 1\n",
    "y_val = dec_val[dim-1:dim-1 + val_len, -horizon] - 1\n",
    "y_test = dec_test[dim-1:dim-1 + test_len, -horizon] - 1\n",
    "\n",
    "# Confirm alignment\n",
    "print(\"Training data shape:\", dec_train[:train_len].shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Validation data shape:\", dec_val[:val_len].shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)\n",
    "print(\"Testing data shape:\", dec_test[:test_len].shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)\n",
    "\n",
    "# Create Dataset instances\n",
    "dataset_train = Dataset(dec_train, y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val, y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test, y_test, num_classes, dim)\n",
    "\n",
    "# Set up DataLoader instances\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify DataLoader functionality with a sample batch\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Sample input batch shape:\", inputs.shape)\n",
    "    print(\"Sample label batch shape:\", labels.shape)\n",
    "    break  # Test with a single batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 203715\n",
      "Dataset length: 50865\n",
      "Dataset length: 139502\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 50 \n",
    "c_final = 4\n",
    "n_heads = 4\n",
    "c_in_axial = 32\n",
    "c_out_axial = 32\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "num_classes = 3\n",
    "\n",
    "# Ensure labels align with data by slicing both consistently\n",
    "horizon = 2\n",
    "dim = 40\n",
    "train_len = dec_train.shape[0] - dim + 1 - horizon\n",
    "val_len = dec_val.shape[0] - dim + 1 - horizon\n",
    "test_len = dec_test.shape[0] - dim + 1 - horizon\n",
    "y_train = dec_train[dim-1:dim-1 + train_len, -horizon] - 1\n",
    "y_val = dec_val[dim-1:dim-1 + val_len, -horizon] - 1\n",
    "y_test = dec_test[dim-1:dim-1 + test_len, -horizon] - 1\n",
    "\n",
    "# Dataset setup\n",
    "dataset_train = Dataset(dec_train[:train_len], y_train, num_classes, dim)\n",
    "dataset_val = Dataset(dec_val[:val_len], y_val, num_classes, dim)\n",
    "dataset_test = Dataset(dec_test[:test_len], y_test, num_classes, dim)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Architecture\n",
    "def _conv1d1x1(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(out_channels)\n",
    "    )\n",
    "\n",
    "class GatedAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads, dim, flag):\n",
    "        super().__init__()\n",
    "        assert (in_channels % heads == 0) and (out_channels % heads == 0)\n",
    "        self.in_channels, self.out_channels, self.heads = in_channels, out_channels, heads\n",
    "        self.dim_head_v = out_channels // heads\n",
    "        self.flag, self.dim = flag, dim\n",
    "        self.dim_head_qk = self.dim_head_v // 2\n",
    "        self.qkv_channels = self.dim_head_v + self.dim_head_qk * 2\n",
    "\n",
    "        self.to_qkv = _conv1d1x1(in_channels, heads * self.qkv_channels)\n",
    "        self.bn_qkv, self.bn_similarity = nn.BatchNorm1d(heads * self.qkv_channels), nn.BatchNorm2d(heads * 3)\n",
    "        self.bn_output = nn.BatchNorm1d(heads * self.qkv_channels)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.f_qr, self.f_kr = nn.Parameter(torch.tensor(0.3), False), nn.Parameter(torch.tensor(0.3), False)\n",
    "        self.f_sve, self.f_sv = nn.Parameter(torch.tensor(0.3), False), nn.Parameter(torch.tensor(0.5), False)\n",
    "\n",
    "        # Position embedding\n",
    "        self.relative = nn.Parameter(torch.randn(self.dim_head_v * 2, dim * 2 - 1), requires_grad=True)\n",
    "        query_index = torch.arange(dim).unsqueeze(0)\n",
    "        key_index = torch.arange(dim).unsqueeze(1)\n",
    "        relative_index = key_index - query_index + dim - 1\n",
    "        self.register_buffer('flatten_index', relative_index.view(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.flag:\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n",
    "        N, W, C, H = x.shape\n",
    "        x = x.view(N * W, C, H)\n",
    "\n",
    "        x = self.to_qkv(x)\n",
    "        qkv = self.bn_qkv(x)\n",
    "        q, k, v = torch.split(qkv.reshape(N * W, self.heads, self.dim_head_v * 2, H),\n",
    "                              [self.dim_head_v // 2, self.dim_head_v // 2, self.dim_head_v], dim=2)\n",
    "\n",
    "        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.dim_head_v * 2, self.dim, self.dim)\n",
    "        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.dim_head_qk, self.dim_head_qk, self.dim_head_v], dim=0)\n",
    "        qr, kr, qk = torch.einsum('bgci,cij->bgij', q, q_embedding), torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3), torch.einsum('bgci, bgcj->bgij', q, k)\n",
    "        qr, kr = torch.mul(qr, self.f_qr), torch.mul(kr, self.f_kr)\n",
    "\n",
    "        similarity = torch.softmax(self.bn_similarity(torch.cat([qk, qr, kr], dim=1)).view(N * W, 3, self.heads, H, H).sum(dim=1), dim=3)\n",
    "        sv, sve = torch.mul(torch.einsum('bgij,bgcj->bgci', similarity, v), self.f_sv), torch.mul(torch.einsum('bgij,cij->bgci', similarity, v_embedding), self.f_sve)\n",
    "\n",
    "        output = self.bn_output(torch.cat([sv, sve], dim=-1).view(N * W, self.out_channels * 2, H)).view(N, W, self.out_channels, 2, H).sum(dim=-2)\n",
    "        return output.permute(0, 2, 3, 1) if not self.flag else output.permute(0, 2, 1, 3)\n",
    "\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, n_heads, pool_kernel, pool_stride):\n",
    "        super().__init__()\n",
    "        self.CNN_in, self.CNN_out = nn.Conv2d(1, c_in, kernel_size=1), nn.Conv2d(c_out, c_final, kernel_size=1)\n",
    "        self.CNN_res2, self.CNN_res1 = nn.Conv2d(c_out, c_final, kernel_size=1), nn.Conv2d(1, c_out, kernel_size=1)\n",
    "        self.norm, self.res_norm2, self.res_norm1, self.norm2 = nn.BatchNorm2d(c_in), nn.BatchNorm2d(c_final), nn.BatchNorm2d(c_out), nn.BatchNorm2d(c_final)\n",
    "        self.axial_height_1, self.axial_width_1 = GatedAxialAttention(c_out, c_out, n_heads, H, False), GatedAxialAttention(c_out, c_out, n_heads, W, True)\n",
    "        self.axial_height_2, self.axial_width_2 = GatedAxialAttention(c_out, c_out, n_heads, H, False), GatedAxialAttention(c_out, c_out, n_heads, W, True)\n",
    "        self.activation, self.linear = nn.ReLU(), nn.Linear(1600, 3)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.activation(self.norm(self.CNN_in(x)))\n",
    "        y, x = self.axial_width_1(y), self.activation(self.res_norm1(self.CNN_res1(x)))\n",
    "        y, y_copy = y + x, y.detach().clone()\n",
    "        y = self.axial_width_2(y + x)\n",
    "        y = self.activation(self.res_norm2(self.CNN_out(self.axial_height_2(self.axial_height_1(y)))))\n",
    "        return torch.softmax(self.linear(torch.flatten(self.pooling(y + self.activation(self.norm2(self.CNN_res2(y_copy)))), 1)), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training data shape: (203767, 40, 40)\n",
      "Training labels shape: (203767,)\n",
      "Validation data shape: (50942, 40, 40)\n",
      "Validation labels shape: (50942,)\n",
      "Testing data shape: (139546, 40, 40)\n",
      "Testing labels shape: (139546,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define paths based on your directory structure for NoAuction\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load the full dataset\n",
    "dec_data = np.loadtxt(train_file)\n",
    "\n",
    "# Set parameters\n",
    "W = 40                     # Number of features\n",
    "dim = 40                   # Number of LOB states (sequence length)\n",
    "horizon = 2                # Horizon for target calculation\n",
    "\n",
    "# Prepare data for model input\n",
    "dec_data = dec_data[:40, :].T  # Shape: (num_samples, features)\n",
    "N = dec_data.shape[0]\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequences = []\n",
    "labels = []\n",
    "for i in range(N - dim - horizon + 1):\n",
    "    seq = dec_data[i:i+dim]  # Sequence of length 'dim'\n",
    "    label = dec_data[i+dim+horizon-1, -horizon]  # Label corresponding to the sequence\n",
    "    sequences.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_full = np.array(sequences)\n",
    "y_full_raw = np.array(labels)\n",
    "\n",
    "# Map labels to classes using fixed thresholds\n",
    "mean_label = np.mean(y_full_raw)\n",
    "std_label = np.std(y_full_raw)\n",
    "lower_threshold = mean_label - std_label\n",
    "upper_threshold = mean_label + std_label\n",
    "\n",
    "# Function to map continuous labels to class indices using fixed thresholds\n",
    "def map_labels_fixed(y, lower_threshold, upper_threshold):\n",
    "    y_mapped = np.zeros_like(y, dtype=int)\n",
    "    y_mapped[y <= lower_threshold] = 0\n",
    "    y_mapped[(y > lower_threshold) & (y <= upper_threshold)] = 1\n",
    "    y_mapped[y > upper_threshold] = 2\n",
    "    return y_mapped\n",
    "\n",
    "# Map labels\n",
    "y_full = map_labels_fixed(y_full_raw, lower_threshold, upper_threshold)\n",
    "\n",
    "# Perform stratified splitting\n",
    "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\n",
    "\n",
    "# Load test data and concatenate\n",
    "dec_test1 = np.loadtxt(test_file1)\n",
    "dec_test2 = np.loadtxt(test_file2)\n",
    "dec_test3 = np.loadtxt(test_file3)\n",
    "\n",
    "# Concatenate and slice to the first 40 features\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "dec_test = dec_test[:40, :].T  # Shape: (num_samples, 40)\n",
    "\n",
    "# Prepare test data\n",
    "N_test = dec_test.shape[0]\n",
    "sequences_test = []\n",
    "labels_test = []\n",
    "for i in range(N_test - dim - horizon + 1):\n",
    "    seq = dec_test[i:i+dim]  # Sequence of shape (40, 40)\n",
    "    label = dec_test[i+dim+horizon-1, -horizon]\n",
    "    sequences_test.append(seq)\n",
    "    labels_test.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_test_seq = np.array(sequences_test, dtype=np.float32)\n",
    "y_test_raw = np.array(labels_test)\n",
    "y_test_seq = map_labels_fixed(y_test_raw, lower_threshold, upper_threshold)\n",
    "\n",
    "# Confirm alignment\n",
    "print(\"Training data shape:\", X_train_seq.shape)\n",
    "print(\"Training labels shape:\", y_train_seq.shape)\n",
    "print(\"Validation data shape:\", X_val_seq.shape)\n",
    "print(\"Validation labels shape:\", y_val_seq.shape)\n",
    "print(\"Testing data shape:\", X_test_seq.shape)\n",
    "print(\"Testing labels shape:\", y_test_seq.shape)\n",
    "\n",
    "# Define Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X_seq = X_seq\n",
    "        self.y_seq = y_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_seq)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.tensor(self.X_seq[index], dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        y = torch.tensor(self.y_seq[index], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Define AxialLOB model with Gated Axial Attention\n",
    "# Include the AxialLOB and GatedAxialAttention classes as provided in your previous code\n",
    "\n",
    "# Model, loss function, optimizer, and scheduler setup\n",
    "model = AxialLOB(W=40, H=40, c_in=c_in_axial, c_out=c_out_axial, c_final=c_final, n_heads=n_heads,\n",
    "                 pool_kernel=pool_kernel, pool_stride=pool_stride).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.00001)\n",
    "\n",
    "# Create Dataset instances\n",
    "dataset_train = Dataset(X_train_seq, y_train_seq)\n",
    "dataset_val = Dataset(X_val_seq, y_val_seq)\n",
    "dataset_test = Dataset(X_test_seq, y_test_seq)\n",
    "\n",
    "# Set up DataLoader instances\n",
    "train_loader = data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define batch_gd function\n",
    "def batch_gd(model, criterion, optimizer, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        train_losses[it] = np.mean(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss.append(loss.item())\n",
    "        val_losses[it] = np.mean(val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model based on validation loss\n",
    "        if val_losses[it] < best_val_loss:\n",
    "            torch.save(model.state_dict(), 'model_best.pth')\n",
    "            best_val_loss = val_losses[it]\n",
    "            best_epoch = it\n",
    "            print('Model saved')\n",
    "\n",
    "        print(f\"Epoch {it+1}/{epochs}, Train Loss: {train_losses[it]:.4f}, \"\n",
    "              f\"Validation Loss: {val_losses[it]:.4f}, Best Val Epoch: {best_epoch+1}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Execute training\n",
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training data shape: (203767, 40, 40)\n",
      "Training labels shape: (203767,)\n",
      "Validation data shape: (50942, 40, 40)\n",
      "Validation labels shape: (50942,)\n",
      "Testing data shape: (139546, 40, 40)\n",
      "Testing labels shape: (139546,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define paths based on your directory structure for NoAuction\n",
    "train_file = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file1 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "test_file2 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "test_file3 = 'C:/Users/monam/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "\n",
    "# Load the full dataset\n",
    "dec_data = np.loadtxt(train_file)\n",
    "\n",
    "# Set parameters\n",
    "W = 40                     # Number of features\n",
    "dim = 40                   # Number of LOB states (sequence length)\n",
    "horizon = 2                # Horizon for target calculation\n",
    "\n",
    "# Prepare data for model input\n",
    "dec_data = dec_data[:40, :].T  # Shape: (num_samples, features)\n",
    "N = dec_data.shape[0]\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequences = []\n",
    "labels = []\n",
    "for i in range(N - dim - horizon + 1):\n",
    "    seq = dec_data[i:i+dim]  # Sequence of length 'dim'\n",
    "    label = dec_data[i+dim+horizon-1, -horizon]  # Label corresponding to the sequence\n",
    "    sequences.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_full = np.array(sequences)\n",
    "y_full_raw = np.array(labels)\n",
    "\n",
    "# Map labels to classes using fixed thresholds\n",
    "mean_label = np.mean(y_full_raw)\n",
    "std_label = np.std(y_full_raw)\n",
    "lower_threshold = mean_label - std_label\n",
    "upper_threshold = mean_label + std_label\n",
    "\n",
    "# Function to map continuous labels to class indices using fixed thresholds\n",
    "def map_labels_fixed(y, lower_threshold, upper_threshold):\n",
    "    y_mapped = np.zeros_like(y, dtype=int)\n",
    "    y_mapped[y <= lower_threshold] = 0\n",
    "    y_mapped[(y > lower_threshold) & (y <= upper_threshold)] = 1\n",
    "    y_mapped[y > upper_threshold] = 2\n",
    "    return y_mapped\n",
    "\n",
    "# Map labels\n",
    "y_full = map_labels_fixed(y_full_raw, lower_threshold, upper_threshold)\n",
    "\n",
    "# Perform stratified splitting\n",
    "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\n",
    "\n",
    "# Load test data and concatenate\n",
    "dec_test1 = np.loadtxt(test_file1)\n",
    "dec_test2 = np.loadtxt(test_file2)\n",
    "dec_test3 = np.loadtxt(test_file3)\n",
    "\n",
    "# Concatenate and slice to the first 40 features\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "dec_test = dec_test[:40, :].T  # Shape: (num_samples, 40)\n",
    "\n",
    "# Prepare test data\n",
    "N_test = dec_test.shape[0]\n",
    "sequences_test = []\n",
    "labels_test = []\n",
    "for i in range(N_test - dim - horizon + 1):\n",
    "    seq = dec_test[i:i+dim]  # Sequence of shape (40, 40)\n",
    "    label = dec_test[i+dim+horizon-1, -horizon]\n",
    "    sequences_test.append(seq)\n",
    "    labels_test.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_test_seq = np.array(sequences_test, dtype=np.float32)\n",
    "y_test_raw = np.array(labels_test)\n",
    "y_test_seq = map_labels_fixed(y_test_raw, lower_threshold, upper_threshold)\n",
    "\n",
    "# Confirm alignment\n",
    "print(\"Training data shape:\", X_train_seq.shape)\n",
    "print(\"Training labels shape:\", y_train_seq.shape)\n",
    "print(\"Validation data shape:\", X_val_seq.shape)\n",
    "print(\"Validation labels shape:\", y_val_seq.shape)\n",
    "print(\"Testing data shape:\", X_test_seq.shape)\n",
    "print(\"Testing labels shape:\", y_test_seq.shape)\n",
    "\n",
    "# Define Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X_seq = X_seq\n",
    "        self.y_seq = y_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_seq)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.tensor(self.X_seq[index], dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        y = torch.tensor(self.y_seq[index], dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# Define AxialLOB model with Gated Axial Attention\n",
    "def _conv1d1x1(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(out_channels)\n",
    "    )\n",
    "\n",
    "class GatedAxialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads, dim, flag):\n",
    "        super().__init__()\n",
    "        assert (in_channels % heads == 0) and (out_channels % heads == 0)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.dim_head_v = out_channels // heads\n",
    "        self.flag = flag\n",
    "        self.dim = dim\n",
    "        self.dim_head_qk = self.dim_head_v // 2\n",
    "        self.qkv_channels = self.dim_head_v + self.dim_head_qk * 2\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.to_qkv = _conv1d1x1(in_channels, self.heads * self.qkv_channels)\n",
    "        self.bn_qkv = nn.BatchNorm1d(self.heads * self.qkv_channels)\n",
    "        self.bn_similarity = nn.BatchNorm2d(heads * 3)\n",
    "        self.bn_output = nn.BatchNorm1d(self.heads * self.qkv_channels)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.f_qr = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_kr = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_sve = nn.Parameter(torch.tensor(0.3), requires_grad=False)\n",
    "        self.f_sv = nn.Parameter(torch.tensor(0.5), requires_grad=False)\n",
    "\n",
    "        # Position embedding\n",
    "        self.relative = nn.Parameter(torch.randn(self.dim_head_v * 2, dim * 2 - 1), requires_grad=True)\n",
    "        query_index = torch.arange(dim).unsqueeze(0)\n",
    "        key_index = torch.arange(dim).unsqueeze(1)\n",
    "        relative_index = key_index - query_index + dim - 1\n",
    "        self.register_buffer('flatten_index', relative_index.view(-1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.relative, 0., math.sqrt(1. / self.dim_head_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.flag:\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            x = x.permute(0, 3, 1, 2)  # N, W, C, H\n",
    "        N, W, C, H = x.shape\n",
    "        x = x.reshape(N * W, C, H)\n",
    "\n",
    "        # Transformations\n",
    "        x = self.to_qkv(x)\n",
    "        qkv = self.bn_qkv(x)\n",
    "        q, k, v = torch.split(qkv.reshape(N * W, self.heads, self.dim_head_v * 2, H),\n",
    "                              [self.dim_head_v // 2, self.dim_head_v // 2, self.dim_head_v], dim=2)\n",
    "\n",
    "        # Calculate position embedding\n",
    "        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).reshape(\n",
    "            self.dim_head_v * 2, self.dim, self.dim)\n",
    "        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings,\n",
    "                                                            [self.dim_head_qk, self.dim_head_qk, self.dim_head_v],\n",
    "                                                            dim=0)\n",
    "        qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n",
    "        kr = torch.einsum('bgci,cij->bgij', k, k_embedding).transpose(2, 3)\n",
    "        qk = torch.einsum('bgci, bgcj->bgij', q, k)\n",
    "\n",
    "        # Multiply by factors\n",
    "        qr = torch.mul(qr, self.f_qr)\n",
    "        kr = torch.mul(kr, self.f_kr)\n",
    "        stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n",
    "        stacked_similarity = self.bn_similarity(stacked_similarity).reshape(N * W, 3, self.heads, H, H).sum(dim=1)\n",
    "        similarity = torch.softmax(stacked_similarity, dim=3)\n",
    "        sv = torch.einsum('bgij,bgcj->bgci', similarity, v)\n",
    "        sve = torch.einsum('bgij,cij->bgci', similarity, v_embedding)\n",
    "\n",
    "        # Multiply by factors\n",
    "        sv = torch.mul(sv, self.f_sv)\n",
    "        sve = torch.mul(sve, self.f_sve)\n",
    "        stacked_output = torch.cat([sv, sve], dim=-1).reshape(N * W, self.out_channels * 2, H)\n",
    "        output = self.bn_output(stacked_output).reshape(N, W, self.out_channels, 2, H).sum(dim=-2)\n",
    "\n",
    "        if self.flag:\n",
    "            output = output.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            output = output.permute(0, 2, 3, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class AxialLOB(nn.Module):\n",
    "    def __init__(self, W, H, c_in, c_out, c_final, n_heads, pool_kernel, pool_stride):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.c_final = c_final\n",
    "\n",
    "        self.CNN_in = nn.Conv2d(in_channels=1, out_channels=c_in, kernel_size=1)\n",
    "        self.CNN_out = nn.Conv2d(in_channels=c_out, out_channels=c_final, kernel_size=1)\n",
    "        self.CNN_res2 = nn.Conv2d(in_channels=c_out, out_channels=c_final, kernel_size=1)\n",
    "        self.CNN_res1 = nn.Conv2d(in_channels=1, out_channels=c_out, kernel_size=1)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(c_in)\n",
    "        self.res_norm2 = nn.BatchNorm2d(c_final)\n",
    "        self.res_norm1 = nn.BatchNorm2d(c_out)\n",
    "        self.norm2 = nn.BatchNorm2d(c_final)\n",
    "        self.axial_height_1 = GatedAxialAttention(c_out, c_out, n_heads, dim=W, flag=False)\n",
    "        self.axial_width_1 = GatedAxialAttention(c_out, c_out, n_heads, dim=W, flag=True)\n",
    "        self.axial_height_2 = GatedAxialAttention(c_out, c_out, n_heads, dim=W, flag=False)\n",
    "        self.axial_width_2 = GatedAxialAttention(c_out, c_out, n_heads, dim=W, flag=True)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear = nn.Linear(1600, num_classes)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.activation(self.norm(self.CNN_in(x)))\n",
    "        y, x_res = self.axial_width_1(y), self.activation(self.res_norm1(self.CNN_res1(x)))\n",
    "        y, y_copy = y + x_res, y.detach().clone()\n",
    "        y = self.axial_width_2(self.axial_height_1(y))\n",
    "        y = self.activation(self.res_norm2(self.CNN_out(self.axial_height_2(y))))\n",
    "        pooled = self.pooling(y + self.activation(self.norm2(self.CNN_res2(y_copy))))\n",
    "        flattened = torch.flatten(pooled, 1)\n",
    "        logits = self.linear(flattened)\n",
    "        return logits\n",
    "\n",
    "# Model, loss function, optimizer, and scheduler setup\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "c_final = 4\n",
    "n_heads = 2\n",
    "c_in_axial = 16\n",
    "c_out_axial = 16\n",
    "pool_kernel = (1, 4)\n",
    "pool_stride = (1, 4)\n",
    "num_classes = 3\n",
    "\n",
    "model = AxialLOB(W=40, H=40, c_in=c_in_axial, c_out=c_out_axial, c_final=c_final, n_heads=n_heads,\n",
    "                 pool_kernel=pool_kernel, pool_stride=pool_stride).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.00001)\n",
    "\n",
    "# Create Dataset instances\n",
    "dataset_train = Dataset(X_train_seq, y_train_seq)\n",
    "dataset_val = Dataset(X_val_seq, y_val_seq)\n",
    "dataset_test = Dataset(X_test_seq, y_test_seq)\n",
    "\n",
    "# Set up DataLoader instances\n",
    "train_loader = data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define batch_gd function\n",
    "def batch_gd(model, criterion, optimizer, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        train_losses[it] = np.mean(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss.append(loss.item())\n",
    "        val_losses[it] = np.mean(val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model based on validation loss\n",
    "        if val_losses[it] < best_val_loss:\n",
    "            torch.save(model.state_dict(), 'model_best.pth')\n",
    "            best_val_loss = val_losses[it]\n",
    "            best_epoch = it\n",
    "            print('Model saved')\n",
    "\n",
    "        print(f\"Epoch {it+1}/{epochs}, Train Loss: {train_losses[it]:.4f}, \"\n",
    "              f\"Validation Loss: {val_losses[it]:.4f}, Best Val Epoch: {best_epoch+1}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Execute training\n",
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
